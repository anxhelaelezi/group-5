{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data entering the stage should be normalized and cleaned (This can be done using a store procedure from raw to stage or using a python script on the files before their insertion into the stage)\n",
    "- Decide which tables are dimensional and which tables are fact tables\n",
    "- Stage tables should be transferred to the hist layer using store procedures\n",
    "- Dimensional tables should be type 2 slowly changing dimensions\n",
    "- Create 3 triggers on after update, after insert and after delete on any tables in the data warehouse\n",
    "- Create 3 views on the hist layer of the data warehouse that filter data\n",
    "- Create 3 views on the hist layer of the data warehouse that aggregate data\n",
    "- Create 3 different store procedures that perform a particular task in the data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MariaDB instead of MySQL due to operating system compatibility issues. These are entirely interchangeable due to MariaDB being a fork of MySQL with the commitment to maintain compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mariadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    conn = mariadb.connect(\n",
    "        user='root',\n",
    "        unix_socket=\"mysql/mysql.sock\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "except mariadb.Error as e:\n",
    "    print(f\"Error connecting to MariaDB: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Database & Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS AnimeDataWarehouse;\")\n",
    "cursor.execute(\"USE AnimeDataWarehouse;\")\n",
    "\n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS stage;\")\n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS hist;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Stage Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table for anime data\n",
    "anime_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.DimAnime (\n",
    "    MAL_ID INT PRIMARY KEY,\n",
    "    Name VARCHAR(255),\n",
    "    English_name VARCHAR(255),\n",
    "    Japanese_name VARCHAR(255),\n",
    "    Type VARCHAR(50),\n",
    "    Episodes INT,\n",
    "    Aired VARCHAR(100),\n",
    "    Premiered VARCHAR(50),\n",
    "    Producers TEXT,\n",
    "    Licensors TEXT,\n",
    "    Studios TEXT,\n",
    "    Source VARCHAR(50),\n",
    "    Duration INT,  -- Duration in seconds\n",
    "    Rating VARCHAR(50),\n",
    "    Score FLOAT,\n",
    "    Ranked INT,\n",
    "    Popularity INT,\n",
    "    Members INT,\n",
    "    Favorites INT,\n",
    "    Watching INT,\n",
    "    Completed INT,\n",
    "    OnHold INT,\n",
    "    Dropped INT,\n",
    "    PlanToWatch INT\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(anime_table_query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table for user data\n",
    "user_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.DimUser (\n",
    "    user_id INT PRIMARY KEY\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(user_table_query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fact table for user-anime interactions\n",
    "fact_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.FactUserAnimeInteractions (\n",
    "    user_id INT,\n",
    "    anime_id INT,\n",
    "    rating INT,\n",
    "    watching_status INT,\n",
    "    watched_episodes INT,\n",
    "    PRIMARY KEY (user_id, anime_id)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(fact_table_query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table for watching statuses\n",
    "status_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.DimWatchingStatus (\n",
    "    status INT PRIMARY KEY,\n",
    "    description VARCHAR(100)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(status_table_query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table for genres\n",
    "genre_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.DimGenre (\n",
    "    genre_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    genre_name VARCHAR(100)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(genre_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Define bridge table between anime and genres\n",
    "bridge_genre_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.BridgeAnimeGenre (\n",
    "    anime_id INT,\n",
    "    genre_id INT,\n",
    "    PRIMARY KEY (anime_id, genre_id)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(bridge_genre_query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table for studios\n",
    "studio_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.DimStudio (\n",
    "    studio_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    studio_name VARCHAR(255)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(studio_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Define bridge table between anime and studios\n",
    "bridge_studio_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stage.BridgeAnimeStudio (\n",
    "    anime_id INT,\n",
    "    studio_id INT,\n",
    "    PRIMARY KEY (anime_id, studio_id)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(bridge_studio_query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking table: DimAnime\n",
      "Schema of DimAnime:\n",
      "  MAL_ID (int(11))\n",
      "  Name (varchar(255))\n",
      "  English_name (varchar(255))\n",
      "  Japanese_name (varchar(255))\n",
      "  Type (varchar(50))\n",
      "  Episodes (int(11))\n",
      "  Aired (varchar(100))\n",
      "  Premiered (varchar(50))\n",
      "  Producers (text)\n",
      "  Licensors (text)\n",
      "  Studios (text)\n",
      "  Source (varchar(50))\n",
      "  Duration (int(11))\n",
      "  Rating (varchar(50))\n",
      "  Score (float)\n",
      "  Ranked (int(11))\n",
      "  Popularity (int(11))\n",
      "  Members (int(11))\n",
      "  Favorites (int(11))\n",
      "  Watching (int(11))\n",
      "  Completed (int(11))\n",
      "  OnHold (int(11))\n",
      "  Dropped (int(11))\n",
      "  PlanToWatch (int(11))\n",
      "Number of rows in DimAnime: 17562\n",
      "Sample rows from DimAnime:\n",
      "  (1, 'Cowboy Bebop', 'Cowboy Bebop', 'カウボーイビバップ', 'TV', 26, 'Apr 3, 1998 to Apr 24, 1999', 'Spring 1998', 'Bandai Visual', 'Funimation, Bandai Entertainment', 'Sunrise', 'Original', 1440, 'R - 17+ (violence & profanity)', 8.78, 28, 39, 1251960, 61971, 105808, 718161, 71513, 26678, 329800)\n",
      "  (5, 'Cowboy Bebop: Tengoku no Tobira', 'Cowboy Bebop:The Movie', 'カウボーイビバップ 天国の扉', 'Movie', 1, 'Sep 1, 2001', '<NA>', 'Sunrise, Bandai Visual', 'Sony Pictures Entertainment', 'Bones', 'Original', 6900, 'R - 17+ (violence & profanity)', 8.39, 159, 518, 273145, 1174, 4143, 208333, 1935, 770, 57964)\n",
      "  (6, 'Trigun', 'Trigun', 'トライガン', 'TV', 26, 'Apr 1, 1998 to Sep 30, 1998', 'Spring 1998', 'Victor Entertainment', 'Funimation, Geneon Entertainment USA', 'Madhouse', 'Manga', 1440, 'PG-13 - Teens 13 or older', 8.24, 266, 201, 558913, 12944, 29113, 343492, 25465, 13925, 146918)\n",
      "  (7, 'Witch Hunter Robin', 'Witch Hunter Robin', 'Witch Hunter ROBIN (ウイッチハンターロビン)', 'TV', 26, 'Jul 2, 2002 to Dec 24, 2002', 'Summer 2002', 'TV Tokyo, Bandai Visual, Dentsu, Victor Entertainment', 'Funimation, Bandai Entertainment', 'Sunrise', 'Original', 1500, 'PG-13 - Teens 13 or older', 7.27, 2481, 1467, 94683, 587, 4300, 46165, 5121, 5378, 33719)\n",
      "  (8, 'Bouken Ou Beet', 'Beet the Vandel Buster', '冒険王ビィト', 'TV', 52, 'Sep 30, 2004 to Sep 29, 2005', 'Fall 2004', 'TV Tokyo, Dentsu', '<NA>', 'Toei Animation', 'Manga', 1380, 'PG - Children', 6.98, 3710, 4369, 13224, 18, 642, 7314, 766, 1108, 3394)\n",
      "\n",
      "Checking table: DimUser\n",
      "Schema of DimUser:\n",
      "  user_id (int(11))\n",
      "Number of rows in DimUser: 0\n",
      "No rows in DimUser.\n",
      "\n",
      "Checking table: FactUserAnimeInteractions\n",
      "Schema of FactUserAnimeInteractions:\n",
      "  user_id (int(11))\n",
      "  anime_id (int(11))\n",
      "  rating (int(11))\n",
      "  watching_status (int(11))\n",
      "  watched_episodes (int(11))\n",
      "Number of rows in FactUserAnimeInteractions: 0\n",
      "No rows in FactUserAnimeInteractions.\n",
      "\n",
      "Checking table: DimWatchingStatus\n",
      "Schema of DimWatchingStatus:\n",
      "  status (int(11))\n",
      "  description (varchar(100))\n",
      "Number of rows in DimWatchingStatus: 0\n",
      "No rows in DimWatchingStatus.\n",
      "\n",
      "Checking table: DimGenre\n",
      "Schema of DimGenre:\n",
      "  genre_id (int(11))\n",
      "  genre_name (varchar(100))\n",
      "Number of rows in DimGenre: 43\n",
      "Sample rows from DimGenre:\n",
      "  (44, 'Action')\n",
      "  (45, 'Adventure')\n",
      "  (46, 'Comedy')\n",
      "  (47, 'Drama')\n",
      "  (48, 'Sci-Fi')\n",
      "\n",
      "Checking table: BridgeAnimeGenre\n",
      "Schema of BridgeAnimeGenre:\n",
      "  anime_id (int(11))\n",
      "  genre_id (int(11))\n",
      "Number of rows in BridgeAnimeGenre: 50198\n",
      "Sample rows from BridgeAnimeGenre:\n",
      "  (1, 44)\n",
      "  (1, 45)\n",
      "  (1, 46)\n",
      "  (1, 47)\n",
      "  (1, 48)\n",
      "\n",
      "Checking table: DimStudio\n",
      "Schema of DimStudio:\n",
      "  studio_id (int(11))\n",
      "  studio_name (varchar(255))\n",
      "Number of rows in DimStudio: 722\n",
      "Sample rows from DimStudio:\n",
      "  (723, 'Sunrise')\n",
      "  (724, 'Bones')\n",
      "  (725, 'Madhouse')\n",
      "  (726, 'Toei Animation')\n",
      "  (727, 'Gallop')\n",
      "\n",
      "Checking table: BridgeAnimeStudio\n",
      "Schema of BridgeAnimeStudio:\n",
      "  anime_id (int(11))\n",
      "  studio_id (int(11))\n",
      "Number of rows in BridgeAnimeStudio: 22590\n",
      "Sample rows from BridgeAnimeStudio:\n",
      "  (1, 1)\n",
      "  (1, 723)\n",
      "  (5, 2)\n",
      "  (5, 724)\n",
      "  (6, 3)\n"
     ]
    }
   ],
   "source": [
    "tables = [\n",
    "    'DimAnime', 'DimUser', 'FactUserAnimeInteractions', \n",
    "    'DimWatchingStatus', 'DimGenre', 'BridgeAnimeGenre', \n",
    "    'DimStudio', 'BridgeAnimeStudio'\n",
    "]\n",
    "\n",
    "def sanity_check_tables(cursor, tables):\n",
    "    for table in tables:\n",
    "        print(f\"\\nChecking table: {table}\")\n",
    "\n",
    "        cursor.execute(f\"DESCRIBE stage.{table};\")\n",
    "        schema = cursor.fetchall()\n",
    "        print(f\"Schema of {table}:\")\n",
    "        for col in schema:\n",
    "            print(f\"  {col[0]} ({col[1]})\")\n",
    "        \n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM stage.{table};\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        print(f\"Number of rows in {table}: {row_count}\")\n",
    "\n",
    "        cursor.execute(f\"SELECT * FROM stage.{table} LIMIT 5;\")\n",
    "        rows = cursor.fetchall()\n",
    "        if rows:\n",
    "            print(f\"Sample rows from {table}:\")\n",
    "            for row in rows:\n",
    "                print(f\"  {row}\")\n",
    "        else:\n",
    "            print(f\"No rows in {table}.\")\n",
    "\n",
    "sanity_check_tables(cursor, tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Cleaning and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing all these steps one file at a time to account for memory problems when transforming and loading the data (animelist is the biggest issue here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using pandas to load the raw data and clean it in preparation for insertion into the stage tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data better, we will explore:\n",
    "- Data type of each column (unlikely to be useful, pandas tends to cast to a generic object type because of non-standard data)\n",
    "- Number of null/NaN/Unknown values in each column\n",
    "- Unique values in each column (top 5 most common values)\n",
    "\n",
    "We're putting them in a table to make it easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anime.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring anime.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in anime_df:\n",
      "Index(['MAL_ID', 'Name', 'Score', 'Genres', 'English name', 'Japanese name',\n",
      "       'Type', 'Episodes', 'Aired', 'Premiered', 'Producers', 'Licensors',\n",
      "       'Studios', 'Source', 'Duration', 'Rating', 'Ranked', 'Popularity',\n",
      "       'Members', 'Favorites', 'Watching', 'Completed', 'On-Hold', 'Dropped',\n",
      "       'Plan to Watch', 'Score-10', 'Score-9', 'Score-8', 'Score-7', 'Score-6',\n",
      "       'Score-5', 'Score-4', 'Score-3', 'Score-2', 'Score-1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "anime_df = pd.read_csv('raw/anime.csv')\n",
    "print(\"Columns in anime_df:\")\n",
    "print(anime_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Null</th>\n",
       "      <th>NAN</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Most Common Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAL_ID</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[48492, 1, 5, 6, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Maou Gakuin no Futekigousha: Shijou Saikyou n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5141</td>\n",
       "      <td>[Unknown, 6.48, 6.3, 6.31, 6.52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genres</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>[Hentai, Music, Comedy, Kids, Kids, Music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English name</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10565</td>\n",
       "      <td>[Unknown, Cyborg 009, Meow Meow Japanese Histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese name</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[Unknown, ゲゲゲの鬼太郎, 僕のヒーローアカデミア, おしりたんてい, 宇宙の騎士...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>[TV, OVA, Movie, Special, ONA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Episodes</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>516</td>\n",
       "      <td>[1, 12, 2, 13, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aired</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>309</td>\n",
       "      <td>[Unknown, 2005, 2004, 2003, 2021 to ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Premiered</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12817</td>\n",
       "      <td>[Unknown, Spring 2017, Fall 2016, Spring 2018,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Producers</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7794</td>\n",
       "      <td>[Unknown, NHK, Pink Pineapple, Sanrio, Bandai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Licensors</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13616</td>\n",
       "      <td>[Unknown, Funimation, Sentai Filmworks, Media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Studios</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7079</td>\n",
       "      <td>[Unknown, Toei Animation, Sunrise, J.C.Staff, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3567</td>\n",
       "      <td>[Original, Manga, Unknown, Visual novel, Game]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Duration</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>[24 min. per ep., 23 min. per ep., 25 min. per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>688</td>\n",
       "      <td>[PG-13 - Teens 13 or older, G - All Ages, PG -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ranked</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1762</td>\n",
       "      <td>[Unknown, 12978.0, 5297.0, 11806.0, 14852.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popularity</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[4161, 11039, 7656, 7944, 13393]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Members</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[142, 120, 104, 126, 135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Favorites</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Watching</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 7, 9, 8, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completed</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 53, 87, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>On-Hold</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 2, 1, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropped</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 35, 36, 34, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plan to Watch</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[26, 33, 25, 21, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-10</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>437</td>\n",
       "      <td>[4.0, 5.0, 3.0, 6.0, 7.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-9</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3167</td>\n",
       "      <td>[Unknown, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-8</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1371</td>\n",
       "      <td>[Unknown, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-7</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>[2.0, 1.0, 3.0, Unknown, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-6</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511</td>\n",
       "      <td>[Unknown, 2.0, 3.0, 4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-5</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>584</td>\n",
       "      <td>[Unknown, 6.0, 5.0, 7.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-4</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>977</td>\n",
       "      <td>[Unknown, 1.0, 2.0, 4.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-3</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1307</td>\n",
       "      <td>[Unknown, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-2</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1597</td>\n",
       "      <td>[Unknown, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score-1</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>459</td>\n",
       "      <td>[4.0, 3.0, 5.0, 6.0, 7.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Data Type  Null  NAN  Unknown  \\\n",
       "MAL_ID            int64     0    0        0   \n",
       "Name             object     0    0        0   \n",
       "Score            object     0    0     5141   \n",
       "Genres           object     0    0       63   \n",
       "English name     object     0    0    10565   \n",
       "Japanese name    object     0    0       48   \n",
       "Type             object     0    0       37   \n",
       "Episodes         object     0    0      516   \n",
       "Aired            object     0    0      309   \n",
       "Premiered        object     0    0    12817   \n",
       "Producers        object     0    0     7794   \n",
       "Licensors        object     0    0    13616   \n",
       "Studios          object     0    0     7079   \n",
       "Source           object     0    0     3567   \n",
       "Duration         object     0    0      555   \n",
       "Rating           object     0    0      688   \n",
       "Ranked           object     0    0     1762   \n",
       "Popularity        int64     0    0        0   \n",
       "Members           int64     0    0        0   \n",
       "Favorites         int64     0    0        0   \n",
       "Watching          int64     0    0        0   \n",
       "Completed         int64     0    0        0   \n",
       "On-Hold           int64     0    0        0   \n",
       "Dropped           int64     0    0        0   \n",
       "Plan to Watch     int64     0    0        0   \n",
       "Score-10         object     0    0      437   \n",
       "Score-9          object     0    0     3167   \n",
       "Score-8          object     0    0     1371   \n",
       "Score-7          object     0    0      503   \n",
       "Score-6          object     0    0      511   \n",
       "Score-5          object     0    0      584   \n",
       "Score-4          object     0    0      977   \n",
       "Score-3          object     0    0     1307   \n",
       "Score-2          object     0    0     1597   \n",
       "Score-1          object     0    0      459   \n",
       "\n",
       "                                       Most Common Unique Values  \n",
       "MAL_ID                                       [48492, 1, 5, 6, 7]  \n",
       "Name           [Maou Gakuin no Futekigousha: Shijou Saikyou n...  \n",
       "Score                           [Unknown, 6.48, 6.3, 6.31, 6.52]  \n",
       "Genres                [Hentai, Music, Comedy, Kids, Kids, Music]  \n",
       "English name   [Unknown, Cyborg 009, Meow Meow Japanese Histo...  \n",
       "Japanese name  [Unknown, ゲゲゲの鬼太郎, 僕のヒーローアカデミア, おしりたんてい, 宇宙の騎士...  \n",
       "Type                              [TV, OVA, Movie, Special, ONA]  \n",
       "Episodes                                       [1, 12, 2, 13, 3]  \n",
       "Aired                     [Unknown, 2005, 2004, 2003, 2021 to ?]  \n",
       "Premiered      [Unknown, Spring 2017, Fall 2016, Spring 2018,...  \n",
       "Producers      [Unknown, NHK, Pink Pineapple, Sanrio, Bandai ...  \n",
       "Licensors      [Unknown, Funimation, Sentai Filmworks, Media ...  \n",
       "Studios        [Unknown, Toei Animation, Sunrise, J.C.Staff, ...  \n",
       "Source            [Original, Manga, Unknown, Visual novel, Game]  \n",
       "Duration       [24 min. per ep., 23 min. per ep., 25 min. per...  \n",
       "Rating         [PG-13 - Teens 13 or older, G - All Ages, PG -...  \n",
       "Ranked              [Unknown, 12978.0, 5297.0, 11806.0, 14852.0]  \n",
       "Popularity                      [4161, 11039, 7656, 7944, 13393]  \n",
       "Members                                [142, 120, 104, 126, 135]  \n",
       "Favorites                                        [0, 1, 2, 3, 4]  \n",
       "Watching                                         [6, 7, 9, 8, 0]  \n",
       "Completed                                     [0, 1, 53, 87, 64]  \n",
       "On-Hold                                          [0, 2, 1, 3, 4]  \n",
       "Dropped                                      [0, 35, 36, 34, 38]  \n",
       "Plan to Watch                               [26, 33, 25, 21, 32]  \n",
       "Score-10                               [4.0, 5.0, 3.0, 6.0, 7.0]  \n",
       "Score-9                            [Unknown, 1.0, 2.0, 3.0, 4.0]  \n",
       "Score-8                            [Unknown, 1.0, 2.0, 3.0, 4.0]  \n",
       "Score-7                            [2.0, 1.0, 3.0, Unknown, 4.0]  \n",
       "Score-6                            [Unknown, 2.0, 3.0, 4.0, 5.0]  \n",
       "Score-5                            [Unknown, 6.0, 5.0, 7.0, 4.0]  \n",
       "Score-4                            [Unknown, 1.0, 2.0, 4.0, 3.0]  \n",
       "Score-3                            [Unknown, 1.0, 2.0, 3.0, 4.0]  \n",
       "Score-2                            [Unknown, 1.0, 2.0, 3.0, 4.0]  \n",
       "Score-1                                [4.0, 3.0, 5.0, 6.0, 7.0]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null/NAN or Unknown values\n",
    "analysis_anime_df = pd.DataFrame({\n",
    "    'Data Type': anime_df.dtypes,\n",
    "    'Null': anime_df.isnull().sum(),\n",
    "    'NAN': anime_df.isna().sum(),\n",
    "    'Unknown': anime_df.isin(['Unknown']).sum(),\n",
    "    'Most Common Unique Values': [anime_df[column].value_counts().index[:5].tolist() for column in anime_df.columns],\n",
    "})\n",
    "\n",
    "analysis_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning anime.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical columns MAL_ID and Name do not have any missing values. The other columns have missing data noted by 'Unknown'. Score-# columns are floats, but also have 'Unknown' values: Score-# indicates number of users who rated the anime (cannot be fractional). Duration is a string written as 'Unknown' or '24 min. per ep.'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df_cleaned = anime_df.copy()\n",
    "\n",
    "# Convert 'Unknown' to NaN in columns that should be numeric\n",
    "num_cols = ['Score', 'Episodes', 'Ranked', 'Popularity', 'Members', 'Favorites', \n",
    "            'Watching', 'Completed', 'On-Hold', 'Dropped', 'Plan to Watch']\n",
    "\n",
    "# Replace 'Unknown' with NaN, then convert to numeric\n",
    "anime_df_cleaned[num_cols] = anime_df_cleaned[num_cols].replace('Unknown', pd.NA).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# We are not setting NaN values to 0 because it will skew the data, better make that\n",
    "# decision later depending on what is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean String Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Unknown' with NaN in string columns\n",
    "str_cols = ['Genres', 'English name', 'Japanese name', 'Type', 'Aired', 'Premiered', \n",
    "            'Producers', 'Licensors', 'Studios', 'Source', 'Duration', 'Rating']\n",
    "\n",
    "anime_df_cleaned[str_cols] = anime_df_cleaned[str_cols].replace('Unknown', pd.NA)\n",
    "\n",
    "# Strip whitespace from all string columns\n",
    "anime_df_cleaned[str_cols] = anime_df_cleaned[str_cols].apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['24 min. per ep.', '1 hr. 55 min.', '25 min. per ep.',\n",
       "       '23 min. per ep.', '27 min. per ep.', '24 min.', '22 min. per ep.',\n",
       "       '1 hr. 44 min.', '1 hr. 27 min.', '1 hr. 22 min.',\n",
       "       '30 min. per ep.', '1 hr. 31 min.', '2 hr. 4 min.',\n",
       "       '1 hr. 45 min.', '52 min.', '1 hr. 59 min.', '2 hr.',\n",
       "       '21 min. per ep.', '14 min. per ep.', '12 min. per ep.',\n",
       "       '34 min. per ep.', '46 min. per ep.', '1 hr. 37 min.',\n",
       "       '20 min. per ep.', '2 hr. 15 min.', '26 min. per ep.',\n",
       "       '28 min. per ep.', '21 min.', '46 min.', '15 min. per ep.',\n",
       "       '2 hr. 5 min.', '29 min. per ep.', '33 min. per ep.',\n",
       "       '1 hr. 24 min.', '48 min.', '1 hr. 19 min.', '25 min.',\n",
       "       '1 hr. 30 min.', '1 hr. 1 min.', '4 min. per ep.',\n",
       "       '7 min. per ep.', '1 hr. 46 min.', '18 min. per ep.',\n",
       "       '1 hr. 41 min.', '11 min. per ep.', '13 min. per ep.', '55 min.',\n",
       "       '1 hr. 21 min.', '1 hr. 2 min.', '37 min. per ep.',\n",
       "       '42 min. per ep.', '37 min.', '19 min.', '1 hr. 33 min.',\n",
       "       '1 hr. 14 min.', '50 min.', '42 min.', '1 hr. 20 min.',\n",
       "       '45 min. per ep.', '1 hr. 39 min.', '1 hr. 38 min.', '26 min.',\n",
       "       '1 hr. 35 min.', '1 hr. 34 min.', '28 min.', '39 min. per ep.',\n",
       "       '40 min.', '5 min. per ep.', '1 hr. 32 min.', '2 hr. 12 min.',\n",
       "       '1 hr. 36 min.', '36 min. per ep.', '30 min.', '1 hr. 49 min.',\n",
       "       '1 hr. 26 min.', '1 hr. 25 min.', '1 hr. 6 min.', '1 hr. 42 min.',\n",
       "       '1 hr. 43 min.', '1 hr. 23 min.', '15 min.', '51 min.',\n",
       "       '31 min. per ep.', '2 hr. 6 min.', '1 hr. 57 min.',\n",
       "       '1 hr. 28 min.', '8 min. per ep.', '1 hr. 51 min.', '22 min.',\n",
       "       '6 min.', '1 hr. 15 min.', '32 min.', '6 min. per ep.', '23 min.',\n",
       "       '1 hr. 17 min.', '31 min.', '35 min. per ep.', '1 min. per ep.',\n",
       "       '3 min. per ep.', '20 min.', '10 min.', '2 hr. 20 min.',\n",
       "       '54 min. per ep.', '16 min. per ep.', '1 hr. 7 min.', '44 min.',\n",
       "       '1 hr. 12 min.', '27 min.', '17 min.', '29 min.',\n",
       "       '50 min. per ep.', '1 hr. 40 min.', '48 min. per ep.', '1 hr.',\n",
       "       '35 min.', '1 hr. 5 min.', '49 min.', '17 min. per ep.',\n",
       "       '10 min. per ep.', '33 min.', '43 min. per ep.', '59 min.',\n",
       "       '9 min. per ep.', '1 hr. 11 min.', '45 min.', '41 min.',\n",
       "       '1 hr. 10 min.', '11 min.', '53 min.', '32 min. per ep.', '5 min.',\n",
       "       '47 min.', '4 min.', '1 hr. 58 min.', '2 hr. 1 min.', '54 min.',\n",
       "       '1 hr. 3 min.', '40 min. per ep.', '1 hr. 48 min.',\n",
       "       '1 hr. 18 min.', '47 min. per ep.', '2 hr. 16 min.', '43 min.',\n",
       "       '52 min. per ep.', '56 min.', '58 min.', '2 hr. 19 min.',\n",
       "       '51 min. per ep.', '2 hr. 8 min.', '2 hr. 10 min.',\n",
       "       '41 min. per ep.', '58 min. per ep.', '1 hr. 50 min.',\n",
       "       '1 hr. 47 min.', '12 min.', '1 hr. 13 min.', '38 min. per ep.',\n",
       "       '1 hr. 5 min. per ep.', '39 min.', '1 hr. 9 min.', '1 hr. per ep.',\n",
       "       '3 min.', '2 hr. 11 min.', '34 min.', '57 min.', '1 hr. 52 min.',\n",
       "       '2 min. per ep.', '16 min.', '13 min.', '38 min.',\n",
       "       '1 hr. 17 min. per ep.', '8 min.', '1 hr. 38 min. per ep.',\n",
       "       '1 hr. 7 min. per ep.', '55 min. per ep.', '1 hr. 29 min.',\n",
       "       '1 hr. 56 min.', '53 min. per ep.', '1 min.', '9 min.', '7 min.',\n",
       "       '1 hr. 35 min. per ep.', '1 hr. 30 min. per ep.',\n",
       "       '49 min. per ep.', '2 hr. 41 min.', '2 hr. 40 min.', '14 min.',\n",
       "       '1 hr. 8 min.', '57 min. per ep.', '2 min.', '1 hr. 53 min.',\n",
       "       '18 min.', '2 hr. 21 min.', '2 hr. 33 min.',\n",
       "       '1 hr. 52 min. per ep.', '1 hr. 4 min.', '2 hr. 32 min.',\n",
       "       '2 hr. 30 min.', '2 hr. 43 min.', '2 hr. 2 min.', '36 min.', <NA>,\n",
       "       '1 hr. 16 min.', '1 hr. 54 min.', '44 min. per ep.', '12 sec.',\n",
       "       '19 min. per ep.', '52 sec.', '30 sec. per ep.',\n",
       "       '1 hr. 2 min. per ep.', '22 sec. per ep.', '2 hr. 3 min.',\n",
       "       '40 sec. per ep.', '2 hr. 42 min.', '32 sec.',\n",
       "       '1 hr. 14 min. per ep.', '39 sec. per ep.', '2 hr. 28 min.',\n",
       "       '2 hr. 27 min.', '2 hr. 7 min.', '15 sec. per ep.', '30 sec.',\n",
       "       '1 hr. 11 min. per ep.', '1 hr. 16 min. per ep.',\n",
       "       '53 sec. per ep.', '38 sec.', '29 sec.', '35 sec.',\n",
       "       '33 sec. per ep.', '2 hr. 36 min.', '34 sec. per ep.',\n",
       "       '2 hr. 17 min.', '1 hr. 15 min. per ep.', '24 sec. per ep.',\n",
       "       '37 sec. per ep.', '31 sec. per ep.', '29 sec. per ep.',\n",
       "       '1 hr. 36 min. per ep.', '31 sec.', '41 sec.', '45 sec. per ep.',\n",
       "       '35 sec. per ep.', '15 sec.', '49 sec. per ep.', '17 sec.',\n",
       "       '54 sec.', '2 hr. 14 min.', '16 sec.', '41 sec. per ep.',\n",
       "       '27 sec.', '42 sec.', '14 sec.', '26 sec. per ep.', '36 sec.',\n",
       "       '37 sec.', '58 sec. per ep.', '1 hr. 24 min. per ep.', '50 sec.',\n",
       "       '45 sec.', '33 sec.', '32 sec. per ep.', '55 sec.', '57 sec.',\n",
       "       '16 sec. per ep.', '40 sec.', '10 sec.', '46 sec.', '7 sec.',\n",
       "       '49 sec.', '39 sec.', '51 sec. per ep.', '57 sec. per ep.',\n",
       "       '10 sec. per ep.', '44 sec. per ep.', '3 sec.', '2 hr. 47 min.',\n",
       "       '25 sec.', '36 sec. per ep.', '20 sec. per ep.', '42 sec. per ep.',\n",
       "       '19 sec.', '18 sec. per ep.', '44 sec.', '20 sec.', '51 sec.',\n",
       "       '28 sec.', '46 sec. per ep.', '56 sec. per ep.', '34 sec.',\n",
       "       '50 sec. per ep.', '58 sec.', '21 sec.', '13 sec.',\n",
       "       '54 sec. per ep.', '23 sec.', '22 sec.', '21 sec. per ep.',\n",
       "       '25 sec. per ep.', '12 sec. per ep.', '38 sec. per ep.',\n",
       "       '14 sec. per ep.', '6 sec. per ep.', '23 sec. per ep.', '47 sec.',\n",
       "       '1 hr. 9 min. per ep.', '24 sec.', '28 sec. per ep.',\n",
       "       '43 sec. per ep.', '1 hr. 27 min. per ep.', '48 sec.', '6 sec.',\n",
       "       '43 sec.', '55 sec. per ep.'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show unique values in anime_df_cleaned duration\n",
    "anime_df_cleaned['Duration'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown to NaN in 'Duration' column\n",
    "anime_df_cleaned['Duration'] = anime_df_cleaned['Duration'].replace('Unknown', pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def convert_duration_to_seconds(duration):\n",
    "    if pd.isna(duration):\n",
    "        return np.nan\n",
    "\n",
    "    hours_pattern = r\"(\\d+)\\s*hr\\.?\"\n",
    "    minutes_pattern = r\"(\\d+)\\s*min\\.?\"\n",
    "    seconds_pattern = r\"(\\d+)\\s*sec\\.?\"\n",
    "    \n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    seconds = 0\n",
    "    \n",
    "    hours_match = re.search(hours_pattern, duration)\n",
    "    minutes_match = re.search(minutes_pattern, duration)\n",
    "    seconds_match = re.search(seconds_pattern, duration)\n",
    "\n",
    "    if hours_match:\n",
    "        hours = int(hours_match.group(1))\n",
    "    if minutes_match:\n",
    "        minutes = int(minutes_match.group(1))\n",
    "    if seconds_match:\n",
    "        seconds = int(seconds_match.group(1))\n",
    "    \n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    17007.000000\n",
      "mean      1472.767096\n",
      "std       1506.872993\n",
      "min          3.000000\n",
      "25%        300.000000\n",
      "50%       1380.000000\n",
      "75%       1620.000000\n",
      "max      10020.000000\n",
      "Name: Duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Duration' to seconds\n",
    "anime_df_cleaned['Duration'] = anime_df_cleaned['Duration'].apply(convert_duration_to_seconds)\n",
    "\n",
    "print(anime_df_cleaned['Duration'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into DimAnime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MAL_ID', 'Name', 'Score', 'Genres', 'English name', 'Japanese name',\n",
       "       'Type', 'Episodes', 'Aired', 'Premiered', 'Producers', 'Licensors',\n",
       "       'Studios', 'Source', 'Duration', 'Rating', 'Ranked', 'Popularity',\n",
       "       'Members', 'Favorites', 'Watching', 'Completed', 'On-Hold', 'Dropped',\n",
       "       'Plan to Watch', 'Score-10', 'Score-9', 'Score-8', 'Score-7', 'Score-6',\n",
       "       'Score-5', 'Score-4', 'Score-3', 'Score-2', 'Score-1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into stage.DimAnime after deleting existing data\n",
    "cursor.execute(\"DELETE FROM stage.DimAnime;\")\n",
    "conn.commit()\n",
    "\n",
    "for index, row in anime_df_cleaned.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO stage.DimAnime (\n",
    "            MAL_ID, Name, English_name, Japanese_name, Type, Episodes, Aired, Premiered,\n",
    "            Producers, Licensors, Studios, Source, Duration, Rating, Score, Ranked,\n",
    "            Popularity, Members, Favorites, Watching, Completed, OnHold, Dropped, PlanToWatch\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        int(row['MAL_ID']), row['Name'], row['English name'], row['Japanese name'], row['Type'],\n",
    "        int(row['Episodes']) if not pd.isna(row['Episodes']) else None,\n",
    "        row['Aired'], row['Premiered'], row['Producers'], row['Licensors'],\n",
    "        row['Studios'], row['Source'], int(row['Duration']) if not pd.isna(row['Duration']) else None,\n",
    "        row['Rating'], float(row['Score']) if not pd.isna(row['Score']) else None,\n",
    "        int(row['Ranked']) if not pd.isna(row['Ranked']) else None,\n",
    "        int(row['Popularity']) if not pd.isna(row['Popularity']) else None,\n",
    "        int(row['Members']) if not pd.isna(row['Members']) else None,\n",
    "        int(row['Favorites']) if not pd.isna(row['Favorites']) else None,\n",
    "        int(row['Watching']) if not pd.isna(row['Watching']) else None,\n",
    "        int(row['Completed']) if not pd.isna(row['Completed']) else None,\n",
    "        int(row['On-Hold']) if not pd.isna(row['On-Hold']) else None,\n",
    "        int(row['Dropped']) if not pd.isna(row['Dropped']) else None,\n",
    "        int(row['Plan to Watch']) if not pd.isna(row['Plan to Watch']) else None\n",
    "    ))\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Number of rows in DimAnime: 17562\n",
      "Rows in anime_df_cleaned: 17562\n",
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for DimAnime\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.DimAnime;\")\n",
    "rows_in_dimanime = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: Number of rows in DimAnime: {rows_in_dimanime}\")\n",
    "print(\"Rows in anime_df_cleaned:\", len(anime_df_cleaned))\n",
    "assert rows_in_dimanime == len(anime_df_cleaned)\n",
    "print(\"Sanity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into DimGenre and BridgeAnimeGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract genres\n",
    "genres_series = anime_df_cleaned['Genres'].dropna().str.split(',').explode().str.strip()\n",
    "unique_genres = genres_series.unique()\n",
    "\n",
    "# Insert into DimGenre after deleting existing data\n",
    "cursor.execute(\"DELETE FROM stage.DimGenre;\")\n",
    "conn.commit()\n",
    "\n",
    "genre_dict = {}\n",
    "for genre in unique_genres:\n",
    "    cursor.execute(\"INSERT INTO stage.DimGenre (genre_name) VALUES (?);\", (genre,))\n",
    "    genre_id = cursor.lastrowid\n",
    "    genre_dict[genre] = genre_id\n",
    "conn.commit()\n",
    "\n",
    "# Create Bridge Table entries\n",
    "bridge_genre_records = []\n",
    "for index, row in anime_df_cleaned.iterrows():\n",
    "    anime_id = int(row['MAL_ID'])\n",
    "    genres = row['Genres']\n",
    "    if pd.notnull(genres):\n",
    "        genre_list = [g.strip() for g in genres.split(',')]\n",
    "        for genre in genre_list:\n",
    "            genre_id = genre_dict.get(genre)\n",
    "            bridge_genre_records.append((anime_id, genre_id))\n",
    "\n",
    "# Delete existing data in BridgeAnimeGenre\n",
    "cursor.execute(\"DELETE FROM stage.BridgeAnimeGenre;\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert into BridgeAnimeGenre\n",
    "insert_bridge_genre_query = \"\"\"\n",
    "INSERT INTO stage.BridgeAnimeGenre (anime_id, genre_id) VALUES (?, ?)\n",
    "\"\"\"\n",
    "cursor.executemany(insert_bridge_genre_query, bridge_genre_records)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Number of rows in BridgeAnimeGenre: 50198\n",
      "Rows in bridge_genre_records: 50198\n",
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for BridgeAnimeGenre\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.BridgeAnimeGenre;\")\n",
    "rows_in_bridge_genre = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: Number of rows in BridgeAnimeGenre: {rows_in_bridge_genre}\")\n",
    "print(\"Rows in bridge_genre_records:\", len(bridge_genre_records))\n",
    "assert rows_in_bridge_genre == len(bridge_genre_records)\n",
    "print(\"Sanity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into DimStudio and BridgeAnimeStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract studios\n",
    "studios_series = anime_df_cleaned['Studios'].dropna().str.split(',').explode().str.strip()\n",
    "unique_studios = studios_series.unique()\n",
    "\n",
    "# Insert into DimStudio after deleting existing data\n",
    "cursor.execute(\"DELETE FROM stage.DimStudio;\")\n",
    "conn.commit()\n",
    "\n",
    "studio_dict = {}\n",
    "for studio in unique_studios:\n",
    "    cursor.execute(\"INSERT INTO stage.DimStudio (studio_name) VALUES (?);\", (studio,))\n",
    "    studio_id = cursor.lastrowid\n",
    "    studio_dict[studio] = studio_id\n",
    "conn.commit()\n",
    "\n",
    "# Create Bridge Table entries\n",
    "bridge_studio_records = []\n",
    "for index, row in anime_df_cleaned.iterrows():\n",
    "    anime_id = int(row['MAL_ID'])\n",
    "    studios = row['Studios']\n",
    "    if pd.notnull(studios):\n",
    "        studio_list = [s.strip() for s in studios.split(',')]\n",
    "        for studio in studio_list:\n",
    "            studio_id = studio_dict.get(studio)\n",
    "            bridge_studio_records.append((anime_id, studio_id))\n",
    "\n",
    "# Insert into BridgeAnimeStudio\n",
    "cursor.execute(\"DELETE FROM stage.BridgeAnimeStudio;\")\n",
    "conn.commit()\n",
    "insert_bridge_studio_query = \"\"\"\n",
    "INSERT INTO stage.BridgeAnimeStudio (anime_id, studio_id) VALUES (?, ?)\n",
    "\"\"\"\n",
    "cursor.executemany(insert_bridge_studio_query, bridge_studio_records)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Number of rows in BridgeAnimeStudio: 11295\n",
      "Rows in bridge_studio_records: 11295\n",
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sanity check for BridgeAnimeStudio\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.BridgeAnimeStudio;\")\n",
    "rows_in_bridge_studio = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: Number of rows in BridgeAnimeStudio: {rows_in_bridge_studio}\")\n",
    "print(\"Rows in bridge_studio_records:\", len(bridge_studio_records))\n",
    "assert rows_in_bridge_studio == len(bridge_studio_records)\n",
    "print(\"Sanity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anime_with_synopsis.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring anime_with_synopsis.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in anime_with_synopsis_df:\n",
      "Index(['MAL_ID', 'Name', 'Score', 'Genres', 'sypnopsis'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# anime_with_synopsis.csv\n",
    "anime_with_synopsis_df = pd.read_csv('raw/anime_with_synopsis.csv')\n",
    "print(\"Columns in anime_with_synopsis_df:\")\n",
    "print(anime_with_synopsis_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Null</th>\n",
       "      <th>NAN</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Most Common Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAL_ID</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[48492, 1, 5, 6, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Maou Gakuin no Futekigousha: Shijou Saikyou n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5123</td>\n",
       "      <td>[Unknown, 6.45, 6.31, 6.52, 6.48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genres</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>[Music, Comedy, Kids, Kids, Music, Dementia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sypnopsis</th>\n",
       "      <td>object</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[No synopsis information has been added to thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Data Type  Null  NAN  Unknown  \\\n",
       "MAL_ID        int64     0    0        0   \n",
       "Name         object     0    0        0   \n",
       "Score        object     0    0     5123   \n",
       "Genres       object     0    0       63   \n",
       "sypnopsis    object     8    8        0   \n",
       "\n",
       "                                   Most Common Unique Values  \n",
       "MAL_ID                                   [48492, 1, 5, 6, 7]  \n",
       "Name       [Maou Gakuin no Futekigousha: Shijou Saikyou n...  \n",
       "Score                      [Unknown, 6.45, 6.31, 6.52, 6.48]  \n",
       "Genres          [Music, Comedy, Kids, Kids, Music, Dementia]  \n",
       "sypnopsis  [No synopsis information has been added to thi...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the data\n",
    "analysis_anime_with_synopsis_df = pd.DataFrame({\n",
    "    'Data Type': anime_with_synopsis_df.dtypes,\n",
    "    'Null': anime_with_synopsis_df.isnull().sum(),\n",
    "    'NAN': anime_with_synopsis_df.isna().sum(),\n",
    "    'Unknown': anime_with_synopsis_df.isin(['Unknown']).sum(),\n",
    "    'Most Common Unique Values': [anime_with_synopsis_df[column].value_counts().index[:5].tolist() for column in anime_with_synopsis_df.columns],\n",
    "})\n",
    "\n",
    "analysis_anime_with_synopsis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning anime_with_synopsis.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16214 entries, 0 to 16213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   MAL_ID    16214 non-null  int64  \n",
      " 1   Name      16214 non-null  object \n",
      " 2   Score     11091 non-null  float64\n",
      " 3   Genres    16151 non-null  object \n",
      " 4   synopsis  16206 non-null  object \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 633.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAL_ID         0\n",
       "Name           0\n",
       "Score       5123\n",
       "Genres        63\n",
       "synopsis       8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle Unknown values in Score and Genres\n",
    "anime_with_synopsis_df.replace({'Genres': {'Unknown': np.nan}}, inplace=True)\n",
    "anime_with_synopsis_df.replace({'Score': {'Unknown': np.nan}}, inplace=True)    \n",
    "\n",
    "# Handle Numeric values\n",
    "anime_with_synopsis_df['Score'] = pd.to_numeric(anime_with_synopsis_df['Score'], errors='coerce')\n",
    "\n",
    "# Handle Synopsis column\n",
    "anime_with_synopsis_df.rename(columns={'sypnopsis': 'synopsis'}, inplace=True)\n",
    "placeholder_text = \"No synopsis information has been added to this title.\"\n",
    "anime_with_synopsis_df.replace({'synopsis': {placeholder_text: np.nan}}, inplace=True)\n",
    "\n",
    "# Drop duplicates and NaN values in critical columns\n",
    "anime_with_synopsis_df.drop_duplicates(inplace=True)\n",
    "anime_with_synopsis_df.dropna(subset=['MAL_ID', 'Name'], inplace=True)\n",
    "\n",
    "\n",
    "print(anime_with_synopsis_df.info())\n",
    "anime_with_synopsis_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### animelist.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring animelist.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# animelist_df = pd.read_csv('raw/animelist.csv', low_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Animelist_df\n",
    "# analysis_animelist_df = pd.DataFrame({\n",
    "#     'Data Type': animelist_df.dtypes,\n",
    "#     'Null': animelist_df.isnull().sum(),\n",
    "#     'NAN': animelist_df.isna().sum(),\n",
    "#     'Unknown': animelist_df.isin(['Unknown']).sum(),\n",
    "#     'Most Common Unique Values': [animelist_df[column].value_counts().index[:5].tolist() for column in animelist_df.columns],\n",
    "# })\n",
    "\n",
    "# analysis_animelist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total unique ratings\n",
    "# animelist_df['rating'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading, Cleaning and Loading unique user IDs into `DimUser` using chunking\n",
    "\n",
    "Read `animelist.csv` in chunks to extract unique user IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 100000 \n",
    "\n",
    "unique_user_ids = set()\n",
    "animelist_chunks = pd.read_csv('raw/animelist.csv', chunksize=chunk_size, usecols=['user_id'], low_memory=True)\n",
    "\n",
    "for chunk in animelist_chunks:\n",
    "    chunk = chunk.dropna(subset=['user_id'])\n",
    "    unique_user_ids.update(chunk['user_id'].astype(int).unique())\n",
    "\n",
    "# Prepare data for insertion into stage.DimUser\n",
    "user_records = [(int(user_id),) for user_id in unique_user_ids]\n",
    "\n",
    "# Insert into DimUser after deleting existing data\n",
    "cursor.execute(\"DELETE FROM stage.DimUser;\")\n",
    "conn.commit()\n",
    "cursor.executemany(\"INSERT INTO stage.DimUser (user_id) VALUES (?);\", user_records)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Number of rows in DimUser: 325770\n",
      "Unique user IDs collected from animelist_df: 325770\n",
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sanity check for DimUser\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.DimUser;\")\n",
    "rows_in_dimuser = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: Number of rows in DimUser: {rows_in_dimuser}\")\n",
    "print(\"Unique user IDs collected from animelist_df:\", len(unique_user_ids))\n",
    "assert rows_in_dimuser == len(unique_user_ids)\n",
    "print(\"Sanity check passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into `FactUserAnimeInteractions` using chunking\n",
    "\n",
    "Read `animelist.csv` in chunks, clean each chunk, and insert the data into the `FactUserAnimeInteractions` table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 150000 records.\n",
      "Processed 300000 records.\n",
      "Processed 450000 records.\n",
      "Processed 600000 records.\n",
      "Processed 750000 records.\n",
      "Processed 900000 records.\n",
      "Processed 1050000 records.\n",
      "Processed 1200000 records.\n",
      "Processed 1350000 records.\n",
      "Processed 1500000 records.\n",
      "Processed 1650000 records.\n",
      "Processed 1800000 records.\n",
      "Processed 1950000 records.\n",
      "Processed 2100000 records.\n",
      "Processed 2250000 records.\n",
      "Processed 2400000 records.\n",
      "Processed 2550000 records.\n",
      "Processed 2700000 records.\n",
      "Processed 2850000 records.\n",
      "Processed 3000000 records.\n",
      "Processed 3150000 records.\n",
      "Processed 3300000 records.\n",
      "Processed 3450000 records.\n",
      "Processed 3600000 records.\n",
      "Processed 3750000 records.\n",
      "Processed 3900000 records.\n",
      "Processed 4050000 records.\n",
      "Processed 4200000 records.\n",
      "Processed 4350000 records.\n",
      "Processed 4500000 records.\n",
      "Processed 4650000 records.\n",
      "Processed 4800000 records.\n",
      "Processed 4950000 records.\n",
      "Processed 5100000 records.\n",
      "Processed 5250000 records.\n",
      "Processed 5400000 records.\n",
      "Processed 5550000 records.\n",
      "Processed 5700000 records.\n",
      "Processed 5850000 records.\n",
      "Processed 6000000 records.\n",
      "Processed 6150000 records.\n",
      "Processed 6300000 records.\n",
      "Processed 6450000 records.\n",
      "Processed 6600000 records.\n",
      "Processed 6750000 records.\n",
      "Processed 6900000 records.\n",
      "Processed 7050000 records.\n",
      "Processed 7200000 records.\n",
      "Processed 7350000 records.\n",
      "Processed 7500000 records.\n",
      "Processed 7650000 records.\n",
      "Processed 7800000 records.\n",
      "Processed 7950000 records.\n",
      "Processed 8100000 records.\n",
      "Processed 8250000 records.\n",
      "Processed 8400000 records.\n",
      "Processed 8550000 records.\n",
      "Processed 8700000 records.\n",
      "Processed 8850000 records.\n",
      "Processed 9000000 records.\n",
      "Processed 9150000 records.\n",
      "Processed 9300000 records.\n",
      "Processed 9450000 records.\n",
      "Processed 9600000 records.\n",
      "Processed 9750000 records.\n",
      "Processed 9900000 records.\n",
      "Processed 10050000 records.\n",
      "Processed 10200000 records.\n",
      "Processed 10350000 records.\n",
      "Processed 10500000 records.\n",
      "Processed 10650000 records.\n",
      "Processed 10800000 records.\n",
      "Processed 10950000 records.\n",
      "Processed 11100000 records.\n",
      "Processed 11250000 records.\n",
      "Processed 11400000 records.\n",
      "Processed 11550000 records.\n",
      "Processed 11700000 records.\n",
      "Processed 11850000 records.\n",
      "Processed 12000000 records.\n",
      "Processed 12150000 records.\n",
      "Processed 12300000 records.\n",
      "Processed 12450000 records.\n",
      "Processed 12600000 records.\n",
      "Processed 12750000 records.\n",
      "Processed 12900000 records.\n",
      "Processed 13050000 records.\n",
      "Processed 13200000 records.\n",
      "Processed 13350000 records.\n",
      "Processed 13500000 records.\n",
      "Processed 13650000 records.\n",
      "Processed 13800000 records.\n",
      "Processed 13950000 records.\n",
      "Processed 14100000 records.\n",
      "Processed 14250000 records.\n",
      "Processed 14400000 records.\n",
      "Processed 14550000 records.\n",
      "Processed 14700000 records.\n",
      "Processed 14850000 records.\n",
      "Processed 15000000 records.\n",
      "Processed 15150000 records.\n",
      "Processed 15300000 records.\n",
      "Processed 15450000 records.\n",
      "Processed 15600000 records.\n",
      "Processed 15750000 records.\n",
      "Processed 15900000 records.\n",
      "Processed 16050000 records.\n",
      "Processed 16200000 records.\n",
      "Processed 16350000 records.\n",
      "Processed 16500000 records.\n",
      "Processed 16650000 records.\n",
      "Processed 16800000 records.\n",
      "Processed 16950000 records.\n",
      "Processed 17100000 records.\n",
      "Processed 17250000 records.\n",
      "Processed 17400000 records.\n",
      "Processed 17550000 records.\n",
      "Processed 17700000 records.\n",
      "Processed 17850000 records.\n",
      "Processed 18000000 records.\n",
      "Processed 18150000 records.\n",
      "Processed 18300000 records.\n",
      "Processed 18450000 records.\n",
      "Processed 18600000 records.\n",
      "Processed 18750000 records.\n",
      "Processed 18900000 records.\n",
      "Processed 19050000 records.\n",
      "Processed 19199999 records.\n",
      "Processed 19349999 records.\n",
      "Processed 19499999 records.\n",
      "Processed 19649999 records.\n",
      "Processed 19799999 records.\n",
      "Processed 19949999 records.\n",
      "Processed 20099999 records.\n",
      "Processed 20249999 records.\n",
      "Processed 20399999 records.\n",
      "Processed 20549999 records.\n",
      "Processed 20699999 records.\n",
      "Processed 20849999 records.\n",
      "Processed 20999999 records.\n",
      "Processed 21149999 records.\n",
      "Processed 21299999 records.\n",
      "Processed 21449999 records.\n",
      "Processed 21599999 records.\n",
      "Processed 21749999 records.\n",
      "Processed 21899999 records.\n",
      "Processed 22049999 records.\n",
      "Processed 22199999 records.\n",
      "Processed 22349999 records.\n",
      "Processed 22499999 records.\n",
      "Processed 22649999 records.\n",
      "Processed 22799999 records.\n",
      "Processed 22949999 records.\n",
      "Processed 23099999 records.\n",
      "Processed 23249999 records.\n",
      "Processed 23399999 records.\n",
      "Processed 23549999 records.\n",
      "Processed 23699999 records.\n",
      "Processed 23849999 records.\n",
      "Processed 23999999 records.\n",
      "Processed 24149999 records.\n",
      "Processed 24299999 records.\n",
      "Processed 24449999 records.\n",
      "Processed 24599999 records.\n",
      "Processed 24749999 records.\n",
      "Processed 24899999 records.\n",
      "Processed 25049999 records.\n",
      "Processed 25199999 records.\n",
      "Processed 25349999 records.\n",
      "Processed 25499999 records.\n",
      "Processed 25649999 records.\n",
      "Processed 25799999 records.\n",
      "Processed 25949999 records.\n",
      "Processed 26099999 records.\n",
      "Processed 26249999 records.\n",
      "Processed 26399999 records.\n",
      "Processed 26549999 records.\n",
      "Processed 26699999 records.\n",
      "Processed 26849999 records.\n",
      "Processed 26999999 records.\n",
      "Processed 27149999 records.\n",
      "Processed 27299999 records.\n",
      "Processed 27449999 records.\n",
      "Processed 27599999 records.\n",
      "Processed 27749999 records.\n",
      "Processed 27899999 records.\n",
      "Processed 28049999 records.\n",
      "Processed 28199999 records.\n",
      "Processed 28349999 records.\n",
      "Processed 28499999 records.\n",
      "Processed 28649999 records.\n",
      "Processed 28799999 records.\n",
      "Processed 28949999 records.\n",
      "Processed 29099999 records.\n",
      "Processed 29249999 records.\n",
      "Processed 29399999 records.\n",
      "Processed 29549999 records.\n",
      "Processed 29699999 records.\n",
      "Processed 29849999 records.\n",
      "Processed 29999999 records.\n",
      "Processed 30149999 records.\n",
      "Processed 30299999 records.\n",
      "Processed 30449999 records.\n",
      "Processed 30599999 records.\n",
      "Processed 30749999 records.\n",
      "Processed 30899999 records.\n",
      "Processed 31049999 records.\n",
      "Processed 31199999 records.\n",
      "Processed 31349999 records.\n",
      "Processed 31499999 records.\n",
      "Processed 31649999 records.\n",
      "Processed 31799999 records.\n",
      "Processed 31949999 records.\n",
      "Processed 32099999 records.\n",
      "Processed 32249999 records.\n",
      "Processed 32399999 records.\n",
      "Processed 32549999 records.\n",
      "Processed 32699999 records.\n",
      "Processed 32849999 records.\n",
      "Processed 32999999 records.\n",
      "Processed 33149999 records.\n",
      "Processed 33299999 records.\n",
      "Processed 33449999 records.\n",
      "Processed 33599999 records.\n",
      "Processed 33749999 records.\n",
      "Processed 33899999 records.\n",
      "Processed 34049999 records.\n",
      "Processed 34199999 records.\n",
      "Processed 34349999 records.\n",
      "Processed 34499999 records.\n",
      "Processed 34649999 records.\n",
      "Processed 34799999 records.\n",
      "Processed 34949999 records.\n",
      "Processed 35099999 records.\n",
      "Processed 35249999 records.\n",
      "Processed 35399999 records.\n",
      "Processed 35549999 records.\n",
      "Processed 35699999 records.\n",
      "Processed 35849999 records.\n",
      "Processed 35999999 records.\n",
      "Processed 36149999 records.\n",
      "Processed 36299999 records.\n",
      "Processed 36449999 records.\n",
      "Processed 36599999 records.\n",
      "Processed 36749999 records.\n",
      "Processed 36899999 records.\n",
      "Processed 37049999 records.\n",
      "Processed 37199999 records.\n",
      "Processed 37349999 records.\n",
      "Processed 37499999 records.\n",
      "Processed 37649999 records.\n",
      "Processed 37799999 records.\n",
      "Processed 37949999 records.\n",
      "Processed 38099999 records.\n",
      "Processed 38249999 records.\n",
      "Processed 38399999 records.\n",
      "Processed 38549999 records.\n",
      "Processed 38699999 records.\n",
      "Processed 38849999 records.\n",
      "Processed 38999999 records.\n",
      "Processed 39149999 records.\n",
      "Processed 39299999 records.\n",
      "Processed 39449999 records.\n",
      "Processed 39599999 records.\n",
      "Processed 39749999 records.\n",
      "Processed 39899999 records.\n",
      "Processed 40049999 records.\n",
      "Processed 40199999 records.\n",
      "Processed 40349999 records.\n",
      "Processed 40499999 records.\n",
      "Processed 40649999 records.\n",
      "Processed 40799999 records.\n",
      "Processed 40949999 records.\n",
      "Processed 41099999 records.\n",
      "Processed 41249999 records.\n",
      "Processed 41399999 records.\n",
      "Processed 41549999 records.\n",
      "Processed 41699999 records.\n",
      "Processed 41849999 records.\n",
      "Processed 41999999 records.\n",
      "Processed 42149999 records.\n",
      "Processed 42299999 records.\n",
      "Processed 42449999 records.\n",
      "Processed 42599999 records.\n",
      "Processed 42749999 records.\n",
      "Processed 42899999 records.\n",
      "Processed 43049999 records.\n",
      "Processed 43199999 records.\n",
      "Processed 43349999 records.\n",
      "Processed 43499999 records.\n",
      "Processed 43649999 records.\n",
      "Processed 43799999 records.\n",
      "Processed 43949999 records.\n",
      "Processed 44099999 records.\n",
      "Processed 44249999 records.\n",
      "Processed 44399999 records.\n",
      "Processed 44549999 records.\n",
      "Processed 44699999 records.\n",
      "Processed 44849999 records.\n",
      "Processed 44999999 records.\n",
      "Processed 45149999 records.\n",
      "Processed 45299999 records.\n",
      "Processed 45449999 records.\n",
      "Processed 45599999 records.\n",
      "Processed 45749999 records.\n",
      "Processed 45899999 records.\n",
      "Processed 46049999 records.\n",
      "Processed 46199999 records.\n",
      "Processed 46349999 records.\n",
      "Processed 46499999 records.\n",
      "Processed 46649999 records.\n",
      "Processed 46799999 records.\n",
      "Processed 46949999 records.\n",
      "Processed 47099999 records.\n",
      "Processed 47249999 records.\n",
      "Processed 47399999 records.\n",
      "Processed 47549999 records.\n",
      "Processed 47699999 records.\n",
      "Processed 47849999 records.\n",
      "Processed 47999999 records.\n",
      "Processed 48149999 records.\n",
      "Processed 48299999 records.\n",
      "Processed 48449999 records.\n",
      "Processed 48599999 records.\n",
      "Processed 48749999 records.\n",
      "Processed 48899999 records.\n",
      "Processed 49049999 records.\n",
      "Processed 49199999 records.\n",
      "Processed 49349999 records.\n",
      "Processed 49499999 records.\n",
      "Processed 49649999 records.\n",
      "Processed 49799999 records.\n",
      "Processed 49949999 records.\n",
      "Processed 50099999 records.\n",
      "Processed 50249999 records.\n",
      "Processed 50399999 records.\n",
      "Processed 50549999 records.\n",
      "Processed 50699999 records.\n",
      "Processed 50849999 records.\n",
      "Processed 50999999 records.\n",
      "Processed 51149999 records.\n",
      "Processed 51299999 records.\n",
      "Processed 51449999 records.\n",
      "Processed 51599999 records.\n",
      "Processed 51749999 records.\n",
      "Processed 51899999 records.\n",
      "Processed 52049999 records.\n",
      "Processed 52199999 records.\n",
      "Processed 52349999 records.\n",
      "Processed 52499999 records.\n",
      "Processed 52649999 records.\n",
      "Processed 52799999 records.\n",
      "Processed 52949999 records.\n",
      "Processed 53099999 records.\n",
      "Processed 53249999 records.\n",
      "Processed 53399999 records.\n",
      "Processed 53549999 records.\n",
      "Processed 53699999 records.\n",
      "Processed 53849999 records.\n",
      "Processed 53999999 records.\n",
      "Processed 54149999 records.\n",
      "Processed 54299999 records.\n",
      "Processed 54449999 records.\n",
      "Processed 54599999 records.\n",
      "Processed 54749999 records.\n",
      "Processed 54899999 records.\n",
      "Processed 55049999 records.\n",
      "Processed 55199999 records.\n",
      "Processed 55349999 records.\n",
      "Processed 55499999 records.\n",
      "Processed 55649999 records.\n",
      "Processed 55799999 records.\n",
      "Processed 55949999 records.\n",
      "Processed 56099999 records.\n",
      "Processed 56249999 records.\n",
      "Processed 56399999 records.\n",
      "Processed 56549999 records.\n",
      "Processed 56699999 records.\n",
      "Processed 56849999 records.\n",
      "Processed 56999999 records.\n",
      "Processed 57149999 records.\n",
      "Processed 57299999 records.\n",
      "Processed 57449999 records.\n",
      "Processed 57599999 records.\n",
      "Processed 57749999 records.\n",
      "Processed 57899999 records.\n",
      "Processed 58049999 records.\n",
      "Processed 58199999 records.\n",
      "Processed 58349999 records.\n",
      "Processed 58499999 records.\n",
      "Processed 58649999 records.\n",
      "Processed 58799999 records.\n",
      "Processed 58949999 records.\n",
      "Processed 59099999 records.\n",
      "Processed 59249999 records.\n",
      "Processed 59399999 records.\n",
      "Processed 59549999 records.\n",
      "Processed 59699999 records.\n",
      "Processed 59849999 records.\n",
      "Processed 59999999 records.\n",
      "Processed 60149999 records.\n",
      "Processed 60299999 records.\n",
      "Processed 60449999 records.\n",
      "Processed 60599999 records.\n",
      "Processed 60749999 records.\n",
      "Processed 60899999 records.\n",
      "Processed 61049999 records.\n",
      "Processed 61199999 records.\n",
      "Processed 61349999 records.\n",
      "Processed 61499999 records.\n",
      "Processed 61649999 records.\n",
      "Processed 61799999 records.\n",
      "Processed 61949999 records.\n",
      "Processed 62099999 records.\n",
      "Processed 62249999 records.\n",
      "Processed 62399999 records.\n",
      "Processed 62549999 records.\n",
      "Processed 62699999 records.\n",
      "Processed 62849999 records.\n",
      "Processed 62999999 records.\n",
      "Processed 63149999 records.\n",
      "Processed 63299999 records.\n",
      "Processed 63449999 records.\n",
      "Processed 63599999 records.\n",
      "Processed 63749999 records.\n",
      "Processed 63899999 records.\n",
      "Processed 64049999 records.\n",
      "Processed 64199999 records.\n",
      "Processed 64349999 records.\n",
      "Processed 64499999 records.\n",
      "Processed 64649999 records.\n",
      "Processed 64799999 records.\n",
      "Processed 64949999 records.\n",
      "Processed 65099999 records.\n",
      "Processed 65249999 records.\n",
      "Processed 65399999 records.\n",
      "Processed 65549999 records.\n",
      "Processed 65699999 records.\n",
      "Processed 65849999 records.\n",
      "Processed 65999999 records.\n",
      "Processed 66149999 records.\n",
      "Processed 66299999 records.\n",
      "Processed 66449999 records.\n",
      "Processed 66599999 records.\n",
      "Processed 66749999 records.\n",
      "Processed 66899999 records.\n",
      "Processed 67049999 records.\n",
      "Processed 67199999 records.\n",
      "Processed 67349999 records.\n",
      "Processed 67499999 records.\n",
      "Processed 67649999 records.\n",
      "Processed 67799999 records.\n",
      "Processed 67949999 records.\n",
      "Processed 68099999 records.\n",
      "Processed 68249999 records.\n",
      "Processed 68399999 records.\n",
      "Processed 68549999 records.\n",
      "Processed 68699999 records.\n",
      "Processed 68849999 records.\n",
      "Processed 68999999 records.\n",
      "Processed 69149999 records.\n",
      "Processed 69299999 records.\n",
      "Processed 69449999 records.\n",
      "Processed 69599999 records.\n",
      "Processed 69749999 records.\n",
      "Processed 69899999 records.\n",
      "Processed 70049999 records.\n",
      "Processed 70199999 records.\n",
      "Processed 70349999 records.\n",
      "Processed 70499999 records.\n",
      "Processed 70649999 records.\n",
      "Processed 70799999 records.\n",
      "Processed 70949999 records.\n",
      "Processed 71099999 records.\n",
      "Processed 71249999 records.\n",
      "Processed 71399999 records.\n",
      "Processed 71549999 records.\n",
      "Processed 71699999 records.\n",
      "Processed 71849999 records.\n",
      "Processed 71999999 records.\n",
      "Processed 72149999 records.\n",
      "Processed 72299999 records.\n",
      "Processed 72449999 records.\n",
      "Processed 72599999 records.\n",
      "Processed 72749999 records.\n",
      "Processed 72899999 records.\n",
      "Processed 73049999 records.\n",
      "Processed 73199999 records.\n",
      "Processed 73349999 records.\n",
      "Processed 73499999 records.\n",
      "Processed 73649999 records.\n",
      "Processed 73799999 records.\n",
      "Processed 73949999 records.\n",
      "Processed 74099999 records.\n",
      "Processed 74249999 records.\n",
      "Processed 74399999 records.\n",
      "Processed 74549999 records.\n",
      "Processed 74699999 records.\n",
      "Processed 74849999 records.\n",
      "Processed 74999999 records.\n",
      "Processed 75149999 records.\n",
      "Processed 75299999 records.\n",
      "Processed 75449999 records.\n",
      "Processed 75599999 records.\n",
      "Processed 75749999 records.\n",
      "Processed 75899999 records.\n",
      "Processed 76049999 records.\n",
      "Processed 76199999 records.\n",
      "Processed 76349999 records.\n",
      "Processed 76499999 records.\n",
      "Processed 76649999 records.\n",
      "Processed 76799999 records.\n",
      "Processed 76949999 records.\n",
      "Processed 77099999 records.\n",
      "Processed 77249999 records.\n",
      "Processed 77399999 records.\n",
      "Processed 77549999 records.\n",
      "Processed 77699999 records.\n",
      "Processed 77849999 records.\n",
      "Processed 77999999 records.\n",
      "Processed 78149999 records.\n",
      "Processed 78299999 records.\n",
      "Processed 78449999 records.\n",
      "Processed 78599999 records.\n",
      "Processed 78749999 records.\n",
      "Processed 78899999 records.\n",
      "Processed 79049999 records.\n",
      "Processed 79199999 records.\n",
      "Processed 79349999 records.\n",
      "Processed 79499999 records.\n",
      "Processed 79649999 records.\n",
      "Processed 79799999 records.\n",
      "Processed 79949999 records.\n",
      "Processed 80099999 records.\n",
      "Processed 80249999 records.\n",
      "Processed 80399999 records.\n",
      "Processed 80549999 records.\n",
      "Processed 80699999 records.\n",
      "Processed 80849999 records.\n",
      "Processed 80999999 records.\n",
      "Processed 81149999 records.\n",
      "Processed 81299999 records.\n",
      "Processed 81449999 records.\n",
      "Processed 81599999 records.\n",
      "Processed 81749999 records.\n",
      "Processed 81899999 records.\n",
      "Processed 82049999 records.\n",
      "Processed 82199999 records.\n",
      "Processed 82349999 records.\n",
      "Processed 82499999 records.\n",
      "Processed 82649999 records.\n",
      "Processed 82799999 records.\n",
      "Processed 82949999 records.\n",
      "Processed 83099999 records.\n",
      "Processed 83249999 records.\n",
      "Processed 83399999 records.\n",
      "Processed 83549999 records.\n",
      "Processed 83699999 records.\n",
      "Processed 83849999 records.\n",
      "Processed 83999999 records.\n",
      "Processed 84149999 records.\n",
      "Processed 84299999 records.\n",
      "Processed 84449999 records.\n",
      "Processed 84599999 records.\n",
      "Processed 84749999 records.\n",
      "Processed 84899999 records.\n",
      "Processed 85049999 records.\n",
      "Processed 85199999 records.\n",
      "Processed 85349999 records.\n",
      "Processed 85499999 records.\n",
      "Processed 85649999 records.\n",
      "Processed 85799999 records.\n",
      "Processed 85949999 records.\n",
      "Processed 86099999 records.\n",
      "Processed 86249999 records.\n",
      "Processed 86399999 records.\n",
      "Processed 86549999 records.\n",
      "Processed 86699999 records.\n",
      "Processed 86849999 records.\n",
      "Processed 86999999 records.\n",
      "Processed 87149999 records.\n",
      "Processed 87299999 records.\n",
      "Processed 87449999 records.\n",
      "Processed 87599999 records.\n",
      "Processed 87749999 records.\n",
      "Processed 87899999 records.\n",
      "Processed 88049999 records.\n",
      "Processed 88199999 records.\n",
      "Processed 88349999 records.\n",
      "Processed 88499999 records.\n",
      "Processed 88649999 records.\n",
      "Processed 88799999 records.\n",
      "Processed 88949999 records.\n",
      "Processed 89099999 records.\n",
      "Processed 89249999 records.\n",
      "Processed 89399999 records.\n",
      "Processed 89549999 records.\n",
      "Processed 89699999 records.\n",
      "Processed 89849999 records.\n",
      "Processed 89999999 records.\n",
      "Processed 90149999 records.\n",
      "Processed 90299999 records.\n",
      "Processed 90449999 records.\n",
      "Processed 90599999 records.\n",
      "Processed 90749999 records.\n",
      "Processed 90899999 records.\n",
      "Processed 91049999 records.\n",
      "Processed 91199999 records.\n",
      "Processed 91349999 records.\n",
      "Processed 91499999 records.\n",
      "Processed 91649999 records.\n",
      "Processed 91799999 records.\n",
      "Processed 91949999 records.\n",
      "Processed 92099999 records.\n",
      "Processed 92249999 records.\n",
      "Processed 92399999 records.\n",
      "Processed 92549999 records.\n",
      "Processed 92699999 records.\n",
      "Processed 92849999 records.\n",
      "Processed 92999999 records.\n",
      "Processed 93149999 records.\n",
      "Processed 93299999 records.\n",
      "Processed 93449999 records.\n",
      "Processed 93599999 records.\n",
      "Processed 93749999 records.\n",
      "Processed 93899999 records.\n",
      "Processed 94049999 records.\n",
      "Processed 94199999 records.\n",
      "Processed 94349999 records.\n",
      "Processed 94499999 records.\n",
      "Processed 94649999 records.\n",
      "Processed 94799999 records.\n",
      "Processed 94949999 records.\n",
      "Processed 95099999 records.\n",
      "Processed 95249999 records.\n",
      "Processed 95399999 records.\n",
      "Processed 95549999 records.\n",
      "Processed 95699999 records.\n",
      "Processed 95849999 records.\n",
      "Processed 95999999 records.\n",
      "Processed 96149999 records.\n",
      "Processed 96299999 records.\n",
      "Processed 96449999 records.\n",
      "Processed 96599999 records.\n",
      "Processed 96749999 records.\n",
      "Processed 96899999 records.\n",
      "Processed 97049999 records.\n",
      "Processed 97199999 records.\n",
      "Processed 97349999 records.\n",
      "Processed 97499999 records.\n",
      "Processed 97649999 records.\n",
      "Processed 97799999 records.\n",
      "Processed 97949999 records.\n",
      "Processed 98099999 records.\n",
      "Processed 98249999 records.\n",
      "Processed 98399999 records.\n",
      "Processed 98549999 records.\n",
      "Processed 98699999 records.\n",
      "Processed 98849999 records.\n",
      "Processed 98999999 records.\n",
      "Processed 99149999 records.\n",
      "Processed 99299999 records.\n",
      "Processed 99449999 records.\n",
      "Processed 99599999 records.\n",
      "Processed 99749999 records.\n",
      "Processed 99899999 records.\n",
      "Processed 100049999 records.\n",
      "Processed 100199999 records.\n",
      "Processed 100349999 records.\n",
      "Processed 100499999 records.\n",
      "Processed 100649999 records.\n",
      "Processed 100799999 records.\n",
      "Processed 100949999 records.\n",
      "Processed 101099999 records.\n",
      "Processed 101249999 records.\n",
      "Processed 101399999 records.\n",
      "Processed 101549999 records.\n",
      "Processed 101699999 records.\n",
      "Processed 101849999 records.\n",
      "Processed 101999999 records.\n",
      "Processed 102149999 records.\n",
      "Processed 102299999 records.\n",
      "Processed 102449999 records.\n",
      "Processed 102599999 records.\n",
      "Processed 102749999 records.\n",
      "Processed 102899999 records.\n",
      "Processed 103049999 records.\n",
      "Processed 103199999 records.\n",
      "Processed 103349999 records.\n",
      "Processed 103499999 records.\n",
      "Processed 103649999 records.\n",
      "Processed 103799999 records.\n",
      "Processed 103949999 records.\n",
      "Processed 104099999 records.\n",
      "Processed 104249999 records.\n",
      "Processed 104399999 records.\n",
      "Processed 104549999 records.\n",
      "Processed 104699999 records.\n",
      "Processed 104849999 records.\n",
      "Processed 104999999 records.\n",
      "Processed 105149999 records.\n",
      "Processed 105299999 records.\n",
      "Processed 105449999 records.\n",
      "Processed 105599999 records.\n",
      "Processed 105749999 records.\n",
      "Processed 105899999 records.\n",
      "Processed 106049999 records.\n",
      "Processed 106199999 records.\n",
      "Processed 106349999 records.\n",
      "Processed 106499999 records.\n",
      "Processed 106649999 records.\n",
      "Processed 106799999 records.\n",
      "Processed 106949999 records.\n",
      "Processed 107099999 records.\n",
      "Processed 107249999 records.\n",
      "Processed 107399999 records.\n",
      "Processed 107549999 records.\n",
      "Processed 107699999 records.\n",
      "Processed 107849999 records.\n",
      "Processed 107999999 records.\n",
      "Processed 108149999 records.\n",
      "Processed 108299999 records.\n",
      "Processed 108449999 records.\n",
      "Processed 108599999 records.\n",
      "Processed 108749999 records.\n",
      "Processed 108899999 records.\n",
      "Processed 109049999 records.\n",
      "Processed 109199999 records.\n",
      "Processed 109224746 records.\n",
      "Total rows inserted: 109224746\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 150000\n",
    "total_rows_inserted = 0\n",
    "\n",
    "animelist_chunks = pd.read_csv('raw/animelist.csv', chunksize=chunk_size, low_memory=True)\n",
    "\n",
    "for chunk in animelist_chunks:\n",
    "    \n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Drop rows with missing necessary values\n",
    "    chunk.dropna(subset=['user_id', 'anime_id', 'watching_status'], inplace=True)\n",
    "    \n",
    "    # Convert numerical columns to ints\n",
    "    chunk[['user_id', 'anime_id', 'rating', 'watching_status', 'watched_episodes']] = chunk[['user_id', 'anime_id', 'rating', 'watching_status', 'watched_episodes']].astype(int)\n",
    "    \n",
    "    # Prepare records and query for insertion\n",
    "    fact_records = chunk[['user_id', 'anime_id', 'rating', 'watching_status', 'watched_episodes']].values.tolist()\n",
    "    \n",
    "    insert_fact_query = \"\"\"\n",
    "    INSERT INTO stage.FactUserAnimeInteractions (\n",
    "        user_id, anime_id, rating, watching_status, watched_episodes\n",
    "    ) VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cursor.executemany(insert_fact_query, fact_records)\n",
    "        conn.commit()\n",
    "        total_rows_inserted += len(fact_records)\n",
    "    except mariadb.Error as e:\n",
    "        print(f\"Error inserting chunk: {e}\")\n",
    "        conn.rollback()\n",
    "    \n",
    "    print(f\"Processed {total_rows_inserted} records.\")\n",
    "\n",
    "print(f\"Total rows inserted: {total_rows_inserted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Number of rows in FactUserAnimeInteractions: 109224746\n",
      "Total rows inserted from animelist_df: 109224746\n",
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sanity check for FactUserAnimeInteractions\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.FactUserAnimeInteractions;\")\n",
    "rows_in_fact = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: Number of rows in FactUserAnimeInteractions: {rows_in_fact}\")\n",
    "print(\"Total rows inserted from animelist_df:\", total_rows_inserted)\n",
    "assert rows_in_fact == total_rows_inserted\n",
    "print(\"Sanity check passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rating_complete.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring rating_complete.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in rating_complete_df:\n",
      "Index(['user_id', 'anime_id', 'rating'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# rating_complete.csv\n",
    "rating_complete_df = pd.read_csv('raw/rating_complete.csv')\n",
    "print(\"Columns in rating_complete_df:\")\n",
    "print(rating_complete_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Null</th>\n",
       "      <th>NAN</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Most Common Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[189037, 162615, 68042, 283786, 259790]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anime_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1535, 16498, 11757, 6547, 30276]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, 7, 9, 6, 10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Type  Null  NAN  Unknown  \\\n",
       "user_id      int64     0    0        0   \n",
       "anime_id     int64     0    0        0   \n",
       "rating       int64     0    0        0   \n",
       "\n",
       "                        Most Common Unique Values  \n",
       "user_id   [189037, 162615, 68042, 283786, 259790]  \n",
       "anime_id        [1535, 16498, 11757, 6547, 30276]  \n",
       "rating                           [8, 7, 9, 6, 10]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the data\n",
    "analysis_rating_complete_df = pd.DataFrame({\n",
    "    'Data Type': rating_complete_df.dtypes,\n",
    "    'Null': rating_complete_df.isnull().sum(),\n",
    "    'NAN': rating_complete_df.isna().sum(),\n",
    "    'Unknown': rating_complete_df.isin(['Unknown']).sum(),\n",
    "    'Most Common Unique Values': [rating_complete_df[column].value_counts().index[:5].tolist() for column in rating_complete_df.columns],\n",
    "})\n",
    "\n",
    "analysis_rating_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57633278 entries, 0 to 57633277\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   anime_id  int64\n",
      " 2   rating    int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 1.3 GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id     0\n",
       "anime_id    0\n",
       "rating      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings are between 1 and 10, ensure that\n",
    "rating_complete_df = rating_complete_df[rating_complete_df['rating'].between(1, 10)]\n",
    "rating_complete_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(rating_complete_df.info())\n",
    "rating_complete_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watching_status.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring watching_status.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in watching_status_df:\n",
      "Index(['status', ' description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "watching_status_df = pd.read_csv('raw/watching_status.csv')\n",
    "print(\"Columns in watching_status_df:\")\n",
    "print(watching_status_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Null</th>\n",
       "      <th>NAN</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Most Common Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Currently Watching, Completed, On Hold, Dropp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Data Type  Null  NAN  Unknown  \\\n",
       "status           int64     0    0        0   \n",
       " description    object     0    0        0   \n",
       "\n",
       "                                      Most Common Unique Values  \n",
       "status                                          [1, 2, 3, 4, 6]  \n",
       " description  [Currently Watching, Completed, On Hold, Dropp...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Watching_status_df\n",
    "analysis_watching_status_df = pd.DataFrame({\n",
    "    'Data Type': watching_status_df.dtypes,\n",
    "    'Null': watching_status_df.isnull().sum(),\n",
    "    'NAN': watching_status_df.isna().sum(),\n",
    "    'Unknown': watching_status_df.isin(['Unknown']).sum(),\n",
    "    'Most Common Unique Values': [watching_status_df[column].value_counts().index[:5].tolist() for column in watching_status_df.columns],\n",
    "})\n",
    "\n",
    "analysis_watching_status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "watching_status contains the information that relates status (int) to it's meaning description (string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean watching_status.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not much to clean here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "watching_status_df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into DimWatchingStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_records = watching_status_df.values.tolist()\n",
    "\n",
    "insert_status_query = \"\"\"\n",
    "INSERT INTO stage.DimWatchingStatus (status, description) VALUES (%s, %s)\n",
    "\"\"\"\n",
    "\n",
    "cursor.executemany(insert_status_query, status_records)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Number of rows in DimWatchingStatus: 5\n",
      "Rows in watching_status_df: 5\n",
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sanity check for DimWatchingStatus\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.DimWatchingStatus;\")\n",
    "rows_in_watchingstatus = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: Number of rows in DimWatchingStatus: {rows_in_watchingstatus}\")\n",
    "print(\"Rows in watching_status_df:\", len(watching_status_df))\n",
    "assert rows_in_watchingstatus == len(watching_status_df)\n",
    "print(\"Sanity check passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hist tables with SCD Type 2 Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dimanime_hist = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hist.DimAnime (\n",
    "    anime_sk INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    MAL_ID INT,\n",
    "    Name VARCHAR(255),\n",
    "    English_name VARCHAR(255),\n",
    "    Japanese_name VARCHAR(255),\n",
    "    Type VARCHAR(50),\n",
    "    Episodes INT,\n",
    "    Aired VARCHAR(100),\n",
    "    Premiered VARCHAR(50),\n",
    "    Producers TEXT,\n",
    "    Licensors TEXT,\n",
    "    Studios TEXT,\n",
    "    Source VARCHAR(50),\n",
    "    Duration INT,\n",
    "    Rating VARCHAR(50),\n",
    "    Score FLOAT,\n",
    "    Ranked INT,\n",
    "    Popularity INT,\n",
    "    Members INT,\n",
    "    Favorites INT,\n",
    "    Watching INT,\n",
    "    Completed INT,\n",
    "    OnHold INT,\n",
    "    Dropped INT,\n",
    "    PlanToWatch INT,\n",
    "    effective_date DATE,\n",
    "    expiry_date DATE,\n",
    "    is_current BOOLEAN,\n",
    "    UNIQUE (MAL_ID, effective_date)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_dimanime_hist)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: 'hist.DimAnime' table created successfully.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SHOW TABLES IN hist LIKE 'DimAnime';\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'hist.DimAnime' table created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hist.DimAnime table has the fields needed for SCD Type 2: \n",
    "- `effective_date`\n",
    "- `expiry_date`\n",
    "- `is_current`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dimuser_hist = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hist.DimUser (\n",
    "    user_sk INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    user_id INT,\n",
    "    effective_date DATE,\n",
    "    expiry_date DATE,\n",
    "    is_current BOOLEAN,\n",
    "    UNIQUE (user_id, effective_date)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_dimuser_hist)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: 'hist.DimUser' table created successfully.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SHOW TABLES IN hist LIKE 'DimUser';\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'hist.DimUser' table created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fact_hist = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hist.FactUserAnimeInteractions (\n",
    "    user_sk INT,\n",
    "    anime_sk INT,\n",
    "    rating INT,\n",
    "    watching_status INT,\n",
    "    watched_episodes INT,\n",
    "    PRIMARY KEY (user_sk, anime_sk),\n",
    "    FOREIGN KEY (user_sk) REFERENCES hist.DimUser(user_sk),\n",
    "    FOREIGN KEY (anime_sk) REFERENCES hist.DimAnime(anime_sk)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_fact_hist)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: 'hist.FactUserAnimeInteractions' table created successfully.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SHOW TABLES IN hist LIKE 'FactUserAnimeInteractions';\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'hist.FactUserAnimeInteractions' table created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we connect users to animes using foreign keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored Procedures to Transfer Data from Stage to Hist (with handling for SCD Type 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hist.DimAnime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP PROCEDURE IF EXISTS hist.SP_InsertUpdate_DimAnime;\")\n",
    "conn.commit()\n",
    "\n",
    "sp_insertupdate_dimanime = \"\"\"\n",
    "CREATE PROCEDURE hist.SP_InsertUpdate_DimAnime()\n",
    "BEGIN\n",
    "    DECLARE done INT DEFAULT FALSE;\n",
    "    DECLARE var_MAL_ID INT;\n",
    "    DECLARE cur_stage CURSOR FOR SELECT MAL_ID FROM stage.DimAnime;\n",
    "    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\n",
    "\n",
    "    OPEN cur_stage;\n",
    "    read_loop: LOOP\n",
    "        FETCH cur_stage INTO var_MAL_ID;\n",
    "        IF done THEN\n",
    "            LEAVE read_loop;\n",
    "        END IF;\n",
    "\n",
    "        IF EXISTS (\n",
    "            SELECT 1 FROM hist.DimAnime\n",
    "            WHERE MAL_ID = var_MAL_ID AND is_current = TRUE\n",
    "        ) THEN\n",
    "            IF EXISTS (\n",
    "                SELECT 1 FROM stage.DimAnime s\n",
    "                JOIN hist.DimAnime h ON s.MAL_ID = h.MAL_ID\n",
    "                WHERE s.MAL_ID = var_MAL_ID AND h.is_current = TRUE AND (\n",
    "                    s.Name <> h.Name OR s.Type <> h.Type OR s.Score <> h.Score\n",
    "                    -- Add other fields as needed\n",
    "                )\n",
    "            ) THEN\n",
    "                -- Expire old record\n",
    "                UPDATE hist.DimAnime\n",
    "                SET expiry_date = CURDATE(), is_current = FALSE\n",
    "                WHERE MAL_ID = var_MAL_ID AND is_current = TRUE;\n",
    "\n",
    "                -- Add new record\n",
    "                INSERT INTO hist.DimAnime (\n",
    "                    MAL_ID, Name, English_name, Japanese_name, Type, Episodes, Aired, Premiered,\n",
    "                    Producers, Licensors, Studios, Source, Duration, Rating, Score, Ranked,\n",
    "                    Popularity, Members, Favorites, Watching, Completed, OnHold, Dropped, PlanToWatch,\n",
    "                    effective_date, expiry_date, is_current\n",
    "                )\n",
    "                SELECT \n",
    "                    s.MAL_ID, s.Name, s.English_name, s.Japanese_name, s.Type, s.Episodes, s.Aired, s.Premiered,\n",
    "                    s.Producers, s.Licensors, s.Studios, s.Source, s.Duration, s.Rating, s.Score, s.Ranked,\n",
    "                    s.Popularity, s.Members, s.Favorites, s.Watching, s.Completed, s.OnHold, s.Dropped, s.PlanToWatch,\n",
    "                    CURDATE(), NULL, TRUE\n",
    "                FROM stage.DimAnime s\n",
    "                WHERE s.MAL_ID = var_MAL_ID;\n",
    "            END IF;\n",
    "        ELSE\n",
    "            -- Add new record\n",
    "            INSERT INTO hist.DimAnime (\n",
    "                MAL_ID, Name, English_name, Japanese_name, Type, Episodes, Aired, Premiered,\n",
    "                Producers, Licensors, Studios, Source, Duration, Rating, Score, Ranked,\n",
    "                Popularity, Members, Favorites, Watching, Completed, OnHold, Dropped, PlanToWatch,\n",
    "                effective_date, expiry_date, is_current\n",
    "            )\n",
    "            SELECT \n",
    "                s.MAL_ID, s.Name, s.English_name, s.Japanese_name, s.Type, s.Episodes, s.Aired, s.Premiered,\n",
    "                s.Producers, s.Licensors, s.Studios, s.Source, s.Duration, s.Rating, s.Score, s.Ranked,\n",
    "                s.Popularity, s.Members, s.Favorites, s.Watching, s.Completed, s.OnHold, s.Dropped, s.PlanToWatch,\n",
    "                CURDATE(), NULL, TRUE\n",
    "            FROM stage.DimAnime s\n",
    "            WHERE s.MAL_ID = var_MAL_ID;\n",
    "        END IF;\n",
    "    END LOOP;\n",
    "    CLOSE cur_stage;\n",
    "END;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(sp_insertupdate_dimanime)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: Stored Procedure 'SP_InsertUpdate_DimAnime' created successfully.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT routine_name FROM information_schema.routines\n",
    "WHERE routine_schema = 'hist' AND routine_name = 'SP_InsertUpdate_DimAnime';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: Stored Procedure 'SP_InsertUpdate_DimAnime' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hist.DimUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP PROCEDURE IF EXISTS hist.SP_InsertUpdate_DimUser;\")\n",
    "conn.commit()\n",
    "\n",
    "sp_insertupdate_dimuser = \"\"\"\n",
    "CREATE PROCEDURE hist.SP_InsertUpdate_DimUser()\n",
    "BEGIN\n",
    "    INSERT INTO hist.DimUser (\n",
    "        user_id, effective_date, expiry_date, is_current\n",
    "    )\n",
    "    SELECT \n",
    "        s.user_id, CURDATE(), NULL, TRUE\n",
    "    FROM stage.DimUser s\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 FROM hist.DimUser h\n",
    "        WHERE s.user_id = h.user_id AND h.is_current = TRUE\n",
    "    );\n",
    "END;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(sp_insertupdate_dimuser)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: Stored Procedure 'SP_InsertUpdate_DimUser' created successfully.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT routine_name FROM information_schema.routines\n",
    "WHERE routine_schema = 'hist' AND routine_name = 'SP_InsertUpdate_DimUser';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: Stored Procedure 'SP_InsertUpdate_DimUser' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hist.FactUserAnimeInteractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP PROCEDURE IF EXISTS hist.SP_Insert_FactUserAnimeInteractions;\")\n",
    "conn.commit()\n",
    "\n",
    "sp_insert_fact = \"\"\"\n",
    "CREATE PROCEDURE hist.SP_Insert_FactUserAnimeInteractions()\n",
    "BEGIN\n",
    "    INSERT INTO hist.FactUserAnimeInteractions (\n",
    "        user_sk, anime_sk, rating, watching_status, watched_episodes\n",
    "    )\n",
    "    SELECT \n",
    "        du.user_sk,\n",
    "        da.anime_sk,\n",
    "        f.rating,\n",
    "        f.watching_status,\n",
    "        f.watched_episodes\n",
    "    FROM stage.FactUserAnimeInteractions f\n",
    "    JOIN hist.DimUser du ON f.user_id = du.user_id AND du.is_current = TRUE\n",
    "    JOIN hist.DimAnime da ON f.anime_id = da.MAL_ID AND da.is_current = TRUE;\n",
    "END\n",
    "\"\"\"\n",
    "cursor.execute(sp_insert_fact)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: Stored Procedure 'SP_Insert_FactUserAnimeInteractions' created successfully.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT routine_name FROM information_schema.routines\n",
    "WHERE routine_schema = 'hist' AND routine_name = 'SP_Insert_FactUserAnimeInteractions';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: Stored Procedure 'SP_Insert_FactUserAnimeInteractions' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute SPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: 325770 records in hist.DimUser, 325770 records in stage.DimUser.\n",
      "Sanity Check Passed: All users transferred to hist.DimUser.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"CALL hist.SP_InsertUpdate_DimUser();\")\n",
    "conn.commit()\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM hist.DimUser WHERE is_current = TRUE;\")\n",
    "hist_dimuser_count = cursor.fetchone()[0]\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.DimUser;\")\n",
    "stage_dimuser_count = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: {hist_dimuser_count} records in hist.DimUser, {stage_dimuser_count} records in stage.DimUser.\")\n",
    "assert hist_dimuser_count == stage_dimuser_count\n",
    "print(\"Sanity Check Passed: All users transferred to hist.DimUser.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: 17562 records in hist.DimAnime, 17562 records in stage.DimAnime.\n",
      "Sanity Check Passed: All animes transferred to hist.DimAnime.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"CALL hist.SP_InsertUpdate_DimAnime();\")\n",
    "conn.commit()\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM hist.DimAnime WHERE is_current = TRUE;\")\n",
    "hist_dimanime_count = cursor.fetchone()[0]\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.DimAnime;\")\n",
    "stage_dimanime_count = cursor.fetchone()[0]\n",
    "print(f\"Sanity Check: {hist_dimanime_count} records in hist.DimAnime, {stage_dimanime_count} records in stage.DimAnime.\")\n",
    "assert hist_dimanime_count == stage_dimanime_count\n",
    "print(\"Sanity Check Passed: All animes transferred to hist.DimAnime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000 \n",
    "offset = 0\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.FactUserAnimeInteractions;\")\n",
    "total_rows_stage = cursor.fetchone()[0]\n",
    "\n",
    "while offset < total_rows_stage:\n",
    "    cursor.execute(f\"\"\"\n",
    "        INSERT INTO hist.FactUserAnimeInteractions (user_sk, anime_sk, rating, watching_status, watched_episodes)\n",
    "        SELECT du.user_sk, da.anime_sk, f.rating, f.watching_status, f.watched_episodes\n",
    "        FROM stage.FactUserAnimeInteractions f\n",
    "        JOIN hist.DimUser du ON f.user_id = du.user_id AND du.is_current = TRUE\n",
    "        JOIN hist.DimAnime da ON f.anime_id = da.MAL_ID AND da.is_current = TRUE\n",
    "        LIMIT {batch_size} OFFSET {offset};\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    offset += batch_size\n",
    "    print(f\"Processed {min(offset, total_rows_stage)} / {total_rows_stage} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sanity check after batching\n",
    "cursor.execute(\"SELECT COUNT(*) FROM hist.FactUserAnimeInteractions;\")\n",
    "hist_fact_count = cursor.fetchone()[0]\n",
    "cursor.execute(\"SELECT COUNT(*) FROM stage.FactUserAnimeInteractions;\")\n",
    "stage_fact_count = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"Sanity Check: {hist_fact_count} records in hist.FactUserAnimeInteractions, {stage_fact_count} records in stage.FactUserAnimeInteractions.\")\n",
    "assert hist_fact_count == stage_fact_count, \"Mismatch between records in hist and stage.\"\n",
    "print(\"Sanity Check Passed: All fact records transferred to hist.FactUserAnimeInteractions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_auditlog = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hist.AuditLog (\n",
    "    audit_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    table_name VARCHAR(100),\n",
    "    operation VARCHAR(10),\n",
    "    record_id INT,\n",
    "    operation_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_auditlog)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SHOW TABLES IN hist LIKE 'AuditLog';\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'hist.AuditLog' table created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Insert Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TRIGGER IF EXISTS hist.DimAnime_AfterInsert;\")\n",
    "conn.commit()\n",
    "\n",
    "trigger_after_insert = \"\"\"\n",
    "CREATE TRIGGER hist.DimAnime_AfterInsert\n",
    "AFTER INSERT ON hist.DimAnime\n",
    "FOR EACH ROW\n",
    "BEGIN\n",
    "    INSERT INTO hist.AuditLog (table_name, operation, record_id)\n",
    "    VALUES ('DimAnime', 'INSERT', NEW.anime_sk);\n",
    "END;\n",
    "\"\"\"\n",
    "cursor.execute(trigger_after_insert)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT TRIGGER_NAME FROM information_schema.triggers\n",
    "WHERE TRIGGER_SCHEMA = 'hist' AND TRIGGER_NAME = 'DimAnime_AfterInsert';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'DimAnime_AfterInsert' trigger created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Update Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TRIGGER IF EXISTS hist.DimAnime_AfterUpdate;\")\n",
    "conn.commit()\n",
    "\n",
    "trigger_after_update = \"\"\"\n",
    "CREATE TRIGGER hist.DimAnime_AfterUpdate\n",
    "AFTER UPDATE ON hist.DimAnime\n",
    "FOR EACH ROW\n",
    "BEGIN\n",
    "    INSERT INTO hist.AuditLog (table_name, operation, record_id)\n",
    "    VALUES ('DimAnime', 'UPDATE', NEW.anime_sk);\n",
    "END;\n",
    "\"\"\"\n",
    "cursor.execute(trigger_after_update)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT TRIGGER_NAME FROM information_schema.triggers\n",
    "WHERE TRIGGER_SCHEMA = 'hist' AND TRIGGER_NAME = 'DimAnime_AfterUpdate';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'DimAnime_AfterUpdate' trigger created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Delete Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TRIGGER IF EXISTS hist.DimAnime_AfterDelete;\")\n",
    "conn.commit()\n",
    "\n",
    "trigger_after_delete = \"\"\"\n",
    "CREATE TRIGGER hist.DimAnime_AfterDelete\n",
    "AFTER DELETE ON hist.DimAnime\n",
    "FOR EACH ROW\n",
    "BEGIN\n",
    "    INSERT INTO hist.AuditLog (table_name, operation, record_id)\n",
    "    VALUES ('DimAnime', 'DELETE', OLD.anime_sk);\n",
    "END;\n",
    "\"\"\"\n",
    "cursor.execute(trigger_after_delete)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT TRIGGER_NAME FROM information_schema.triggers\n",
    "WHERE TRIGGER_SCHEMA = 'hist' AND TRIGGER_NAME = 'DimAnime_AfterDelete';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'DimAnime_AfterDelete' trigger created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FV1 - High-Scoring Anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP VIEW IF EXISTS hist.View_HighScoreAnimes;\")\n",
    "conn.commit()\n",
    "\n",
    "view_high_score_animes = \"\"\"\n",
    "CREATE VIEW hist.View_HighScoreAnimes AS\n",
    "SELECT * FROM hist.DimAnime\n",
    "WHERE Score >= 9.0 AND is_current = TRUE;\n",
    "\"\"\"\n",
    "cursor.execute(view_high_score_animes)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT TABLE_NAME FROM information_schema.VIEWS\n",
    "WHERE TABLE_SCHEMA = 'hist' AND TABLE_NAME = 'View_HighScoreAnimes';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None, \"Sanity Check Failed: 'View_HighScoreAnimes' view was not created.\"\n",
    "print(\"Sanity Check Passed: 'View_HighScoreAnimes' view created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the view returns data\n",
    "cursor.execute(\"SELECT COUNT(*) FROM hist.View_HighScoreAnimes;\")\n",
    "count = cursor.fetchone()[0]\n",
    "assert count >= 0, \"Sanity Check Failed: 'View_HighScoreAnimes' query failed.\"\n",
    "print(f\"'View_HighScoreAnimes' contains {count} records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FV2 - Recent Anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP VIEW IF EXISTS hist.View_RecentAnimes;\")\n",
    "conn.commit()\n",
    "\n",
    "view_recent_animes = \"\"\"\n",
    "CREATE VIEW hist.View_RecentAnimes AS\n",
    "SELECT * FROM hist.DimAnime\n",
    "WHERE effective_date >= DATE_SUB(CURDATE(), INTERVAL 1 YEAR) AND is_current = TRUE;\n",
    "\"\"\"\n",
    "cursor.execute(view_recent_animes)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FV3 - Anime by Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP VIEW IF EXISTS hist.View_AnimesByStudio;\")\n",
    "conn.commit()\n",
    "\n",
    "view_animes_by_studio = \"\"\"\n",
    "CREATE VIEW hist.View_AnimesByStudio AS\n",
    "SELECT a.*\n",
    "FROM hist.DimAnime a\n",
    "JOIN stage.BridgeAnimeStudio bas ON a.MAL_ID = bas.anime_id\n",
    "JOIN stage.DimStudio s ON bas.studio_id = s.studio_id\n",
    "WHERE s.studio_name = 'Studio Ghibli' AND a.is_current = TRUE;\n",
    "\"\"\"\n",
    "cursor.execute(view_animes_by_studio)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AV1 - Avg Anime Score by Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP VIEW IF EXISTS hist.View_AverageScorePerGenre;\")\n",
    "conn.commit()\n",
    "\n",
    "view_avg_score_genre = \"\"\"\n",
    "CREATE VIEW hist.View_AverageScorePerGenre AS\n",
    "SELECT g.genre_name, AVG(a.Score) AS average_score\n",
    "FROM hist.DimGenre g\n",
    "JOIN stage.BridgeAnimeGenre bag ON g.genre_id = bag.genre_id\n",
    "JOIN hist.DimAnime a ON bag.anime_id = a.MAL_ID\n",
    "WHERE a.is_current = TRUE\n",
    "GROUP BY g.genre_name;\n",
    "\"\"\"\n",
    "cursor.execute(view_avg_score_genre)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT TABLE_NAME FROM information_schema.VIEWS\n",
    "WHERE TABLE_SCHEMA = 'hist' AND TABLE_NAME = 'View_AverageScorePerGenre';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'View_AverageScorePerGenre' view created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AV2 - Total people in each watching status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP VIEW IF EXISTS hist.View_TotalMembersByStatus;\")\n",
    "conn.commit()\n",
    "\n",
    "view_total_members_status = \"\"\"\n",
    "CREATE VIEW hist.View_TotalMembersByStatus AS\n",
    "SELECT w.description, SUM(a.Members) AS total_members\n",
    "FROM stage.DimWatchingStatus w\n",
    "JOIN hist.FactUserAnimeInteractions f ON w.status = f.watching_status\n",
    "JOIN hist.DimAnime a ON f.anime_sk = a.anime_sk\n",
    "WHERE a.is_current = TRUE\n",
    "GROUP BY w.description;\n",
    "\"\"\"\n",
    "cursor.execute(view_total_members_status)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AV3 - Top Studios by Anime Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP VIEW IF EXISTS hist.View_TopStudiosByAnimeCount;\")\n",
    "conn.commit()\n",
    "\n",
    "view_top_studios = \"\"\"\n",
    "CREATE VIEW hist.View_TopStudiosByAnimeCount AS\n",
    "SELECT s.studio_name, COUNT(bas.anime_id) AS anime_count\n",
    "FROM stage.DimStudio s\n",
    "JOIN stage.BridgeAnimeStudio bas ON s.studio_id = bas.studio_id\n",
    "GROUP BY s.studio_name\n",
    "ORDER BY anime_count DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "cursor.execute(view_top_studios)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored Procedures for Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SP 1 - Refresh Genre Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_genrestats = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hist.GenreStats (\n",
    "    genre_name VARCHAR(100),\n",
    "    anime_count INT,\n",
    "    average_score FLOAT,\n",
    "    PRIMARY KEY (genre_name)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_genrestats)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SHOW TABLES IN hist LIKE 'GenreStats';\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: 'hist.GenreStats' table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP PROCEDURE IF EXISTS hist.SP_RefreshGenreStats;\")\n",
    "conn.commit()\n",
    "\n",
    "sp_refresh_genre_stats = \"\"\"\n",
    "CREATE PROCEDURE hist.SP_RefreshGenreStats()\n",
    "BEGIN\n",
    "    DELETE FROM hist.GenreStats;\n",
    "    \n",
    "    INSERT INTO hist.GenreStats (genre_name, anime_count, average_score)\n",
    "    SELECT g.genre_name, COUNT(a.anime_sk), AVG(a.Score)\n",
    "    FROM hist.DimGenre g\n",
    "    JOIN stage.BridgeAnimeGenre bag ON g.genre_id = bag.genre_id\n",
    "    JOIN hist.DimAnime a ON bag.anime_id = a.MAL_ID\n",
    "    WHERE a.is_current = TRUE\n",
    "    GROUP BY g.genre_name;\n",
    "END;\n",
    "\"\"\"\n",
    "cursor.execute(sp_refresh_genre_stats)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT ROUTINE_NAME FROM information_schema.ROUTINES\n",
    "WHERE ROUTINE_SCHEMA = 'hist' AND ROUTINE_NAME = 'SP_RefreshGenreStats';\n",
    "\"\"\")\n",
    "result = cursor.fetchone()\n",
    "assert result is not None\n",
    "print(\"Sanity Check Passed: Stored Procedure 'SP_RefreshGenreStats' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SP 2 - Refresh Anime Popularity Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP PROCEDURE IF EXISTS hist.SP_UpdatePopularityRankings;\")\n",
    "conn.commit()\n",
    "\n",
    "sp_update_popularity = \"\"\"\n",
    "DELIMITER $$\n",
    "\n",
    "CREATE PROCEDURE hist.SP_UpdatePopularityRankings()\n",
    "BEGIN\n",
    "    SET @rank = 0;\n",
    "    \n",
    "    UPDATE hist.DimAnime a\n",
    "    JOIN (\n",
    "        SELECT anime_sk, (@rank := @rank + 1) AS popularity_rank\n",
    "        FROM hist.DimAnime\n",
    "        WHERE is_current = TRUE\n",
    "        ORDER BY Members DESC\n",
    "    ) ranks ON a.anime_sk = ranks.anime_sk\n",
    "    SET a.Popularity = ranks.popularity_rank;\n",
    "END $$\n",
    "\n",
    "DELIMITER ;\n",
    "\"\"\"\n",
    "cursor.execute(sp_update_popularity)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SP 3 - Archive for Expired Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hist.Archived_DimAnime table\n",
    "create_archived_dimanime = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hist.Archived_DimAnime LIKE hist.DimAnime;\n",
    "\"\"\"\n",
    "cursor.execute(create_archived_dimanime)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP PROCEDURE IF EXISTS hist.SP_ArchiveExpiredDimensions;\")\n",
    "conn.commit()\n",
    "\n",
    "sp_archive_expired = \"\"\"\n",
    "DELIMITER $$\n",
    "\n",
    "CREATE PROCEDURE hist.SP_ArchiveExpiredDimensions()\n",
    "BEGIN\n",
    "    INSERT INTO hist.Archived_DimAnime\n",
    "    SELECT * FROM hist.DimAnime\n",
    "    WHERE is_current = FALSE AND expiry_date < DATE_SUB(CURDATE(), INTERVAL 1 YEAR);\n",
    "    \n",
    "    DELETE FROM hist.DimAnime\n",
    "    WHERE is_current = FALSE AND expiry_date < DATE_SUB(CURDATE(), INTERVAL 1 YEAR);\n",
    "END $$\n",
    "\n",
    "DELIMITER ;\n",
    "\"\"\"\n",
    "cursor.execute(sp_archive_expired)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
